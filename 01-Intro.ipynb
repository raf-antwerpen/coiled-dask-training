{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "__Welcome!__\n",
    "\n",
    "In a short period of time, we are going to take a comprehensive journey from using Python to scaling and operating large Python clusters with Dask. \n",
    "\n",
    "Our work will include... \n",
    "* querying data, \n",
    "* transforming data for reporting or machine learning purposes\n",
    "* running typical machine learning jobs\n",
    "* creating custom data science applications like simulations\n",
    "* getting familiar with the nuts and bolts of standing up Dask clusters\n",
    "* putting down that wrench and running Dask the easy way\n",
    "* understanding what really happens inside the machinery\n",
    "* learning all about those futuristic dashboards that help us run things\n",
    "* absorbing best practices, hints, and tips that we can use and share with our teams\n",
    "\n",
    "## Instructor and Admin Details\n",
    "\n",
    "### Adam Breindel\n",
    "\n",
    "<img src='images/med-head.jpg' width=250>\n",
    "\n",
    "__LinkedIn__ - https://www.linkedin.com/in/adbreind\n",
    "\n",
    "__Email__ - adam@coiled.io\n",
    "\n",
    "__Twitter__ - <tt>@adbreind</tt>\n",
    "\n",
    "* 20+ years building systems for startups and large enterprises\n",
    "* 10+ years teaching front- and back-end technology\n",
    "\n",
    "__Fun large-scale data projects...__\n",
    "* Streaming neural net + decision tree fraud scoring\n",
    "* Realtime & offline analytics for banking\n",
    "* Music synchronization and licensing for networked jukeboxes\n",
    "\n",
    "__Industries__\n",
    "-   Finance / Insurance\n",
    "-   Travel, Media / Entertainment\n",
    "-   Energy, Government\n",
    "-   Advertising/Social Media, & more\n",
    "\n",
    "### Class Logistics\n",
    "\n",
    "* Main schedule (dates/times)\n",
    "* Breaks\n",
    "\n",
    "We totally understand how challenging it is to try and focus/absorb a ton of new stuff for hours at a time in front of a screen, especially since your \"regular\" job may not stop to give you the time off.\n",
    "\n",
    "__What am I expected to know already?__\n",
    "\n",
    "Maybe some Python and a few parts of the SciPy/PyData stack. *But if we cover an area you area you are not familiar with, please don't hesitate to ask about it!* __PyData is huge and no one knows all of it!__ Everyone's work is different, so their expertise and backgrounds are different. This is a __good__ thing and there are __no bad questions__.\n",
    "\n",
    "### Materials\n",
    "\n",
    "Everything we do is in these notebooks (and accompanying data, etc.) -- no PowerPoint or PDFs!\n",
    "\n",
    "In addition to having hands-on, runnable code throughout, we'll also have a handful of slightly longer labs (usually around 10-20 minutes) so that you can try some mini-projects on your own.\n",
    "\n",
    "For the class we'll use cloud-based versions, so no need to install!\n",
    "\n",
    "__You May Want to Get/Keep the Materials for Review Later__\n",
    "\n",
    "You can download a .zip file with the full contents (including an \"environment\" file that lists all dependencies). I'll supply the URL in the live chat. That zip file will be up permanently, so you don't need to rush; you can get it any time.\n",
    "\n",
    "*Running the Environment Locally*\n",
    "\n",
    "The easiest way to get everything set up locally (and also keep everything encapsulated, so it won't mess with your other Python projects) is to download and install Anaconda (or Miniconda) from https://www.anaconda.com/distribution/\n",
    "\n",
    "Anaconda lets you easily manage multiple, isolated Python environments, each with their own dependencies.\n",
    "\n",
    "Once you have Anaconda set up, the basic way to install the environment is to create a new conda env for this class with the --file option and point at binder/environment.yml\n",
    "\n",
    "If you're in a hurry, there's a Conda cheat sheet with basic details at https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "Now that your conda environment is set up, and you've switched to it, you probably want to install the Dask JupyterLab extension, described here: https://docs.coiled.io/user_guide/jupyter.html#dask-jupyterlab-extension\n",
    "\n",
    "Once you're in the root of the course materials folder, you can run `jupyter lab` at the command line to launch a browser that should come up very much like this one!\n",
    "\n",
    "*Running the environment in the cloud with Coiled Cloud*\n",
    "\n",
    "We'll discuss this further in class, but we would love for you to work with these materials -- and your own compute challenges -- on our Coiled Cloud. We want to make this easy for you and free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Tooling\n",
    "\n",
    "Although our focus is on Dask, we'll be using JupyterLab (this notebook environment) as our \"front-end\" interface. We'll explain more details as we go, but here are a couple of minimal \"survival guide\" style tips:\n",
    "\n",
    "* To run code in a notebook cell, select the cell with keyboard or mouse and then hit CTRL+ENTER\n",
    "    * There are other ways to run code, and you're welcome to use them, but this is the simplest way to get started\n",
    "* To insert a new cell, either\n",
    "    * Hit ESC and then B\n",
    "        * Hint: the ESC is only necessary if you're in \"editing mode\" in a cell, otherwise you can skip it\n",
    "    * or click the '+' toolbar icon at the top of the notebook\n",
    "\n",
    "__Try It!__\n",
    "\n",
    "Insert a cell, enter a very serious computation like `3+4` and then execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascadia City\n",
    "\n",
    "In this class, we'll organize our activities around real data and a realistic (though fictional) scenario. We've been recruited by the data and planning office of Cascadia City, somewhere in the Pacfic Northwest.\n",
    "\n",
    "<img src='images/forest.jpg'>\n",
    "\n",
    "We will work on a number of proof-of-concept projects to see how Dask can help the city analyze data and solve problems.\n",
    "\n",
    "Since Cascadia City may grow to be a bit like Seattle, Washington, we'll use a number of public datasets from Seattle to facilitate our work as well as a some fictional data, including\n",
    "* Library loans\n",
    "* Pet registrations\n",
    "* Fire risk and emergency response\n",
    "* Land use data\n",
    "* Imagery\n",
    "\n",
    "__In the first session(s)__ we'll wear our data analyst and data science hats, and tackle user-facing use cases: we'll learn to use Dask's APIs to solve problems for the city\n",
    "\n",
    "__In later session(s)__ we'll put on our ops and support hats, and make sure we can really understand how Dask works, how we can deploy it for the rest of the team, and how to debug and troubleshoot so we can help them (and ourselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask: Scaling Python Simply\n",
    "\n",
    "Dask is a distributed compute system for Python which scales efficiently from a single laptop up to thousands of servers. Dask is developed in ongoing collaboration with the PyData community so that it is easy to learn, integrate, and operate. Dask leverages regular Python code to scale the work you already do using the skills you already have.\n",
    "\n",
    "### Why scale?\n",
    "\n",
    "In a nutshell,\n",
    "* In the 1900s, computer processors got faster: they ran more and more instructions per second\n",
    "* But, in the 2000s, due to a variety of engineering limitations, we can no longer acquire strictly faster processors. Instead we use more processors in collaboration\n",
    "* Many computations are faster if we can hold a dataset in local memory: while individual computers with huge memory do exist, it's usually easier and cheaper to use a collection of computers and \"pool\" their memory to store our data\n",
    "\n",
    "### Why Python?\n",
    "\n",
    "There are lots of wonderful languages, and we don't get \"religious\" about any of them. However,\n",
    "* There is a huge and popular ecosystem of data and science tooling in Python, and if that works for you, you've come to the right place\n",
    "* Python is easy and fast to read and write, so it supports a high-productivity workflow ... provided we can compute fast enough\n",
    "\n",
    "### Why simple?\n",
    "\n",
    "If you are an academic, professional, or hobbyist computer science researcher, you may want to investigate extremely complex systems that can do unusual and clever things.\n",
    "\n",
    "But, for most users, dealing with the complexity of our computing tools is a headwind that we would like to avoid. Especially if that complexity seems hidden for a while ... and then suddenly overwhelms us when we need to deal with an edge case, or debug, or engage in performance tuning.\n",
    "\n",
    "### Where are the docs?\n",
    "\n",
    "We'll provide more links to documentation as we go, but here's a quick list you can refer to:\n",
    "* Main project page https://dask.org/\n",
    "* Core documentation https://docs.dask.org/en/latest/\n",
    "* Distributed (scheduler) https://distributed.dask.org/en/latest/\n",
    "* Machine learning https://ml.dask.org/\n",
    "* Deployment tools\n",
    "    * Kubernetes https://kubernetes.dask.org/en/latest/\n",
    "    * AWS or Azure https://cloudprovider.dask.org/en/latest/\n",
    "    * YARN https://yarn.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it like to use Dask?\n",
    "\n",
    "There are 3 main ways that people use Dask, and you can use any combination or all of them.\n",
    "\n",
    "We'll take a quick test drive and see each of the 3 approaches\n",
    "* One-liners (or sometimes \"zero-liners\") where a tool already has Dask integration built in\n",
    "* Dask large-scale datastructures like Dask Dataframe: a scalable Pandas dataframe\n",
    "* Parallelizing custom computation: use the Dask engine to power your own code\n",
    "\n",
    "Let's connect to our hosted Coiled cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've got your first Dask cluster.\n",
    "\n",
    "But how can we be sure it's alive and has the right specs?\n",
    "\n",
    "__Workers Dashboard__\n",
    "\n",
    "Let's take a look at the Workers dashboard panel\n",
    "* Click the Dask logo in the JupyterLab side toolbar\n",
    "* Click the &#x1F50D; icon to the right of the text box\n",
    "* Click the Workers button\n",
    "\n",
    "You should get a tab with a live, animated chart that looks something like this:\n",
    "\n",
    "<img src='images/workers.png' width=700>\n",
    "\n",
    "You can drag/position/snap that in JupyterLab, so that it's visible while you're coding.\n",
    "\n",
    "What are these \"workers\"? Just regular Python processes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try a \"one-liner\" ML application using Dask. We'll run an example from TPOT, an AutoML tool, to classify the \"digits\" dataset from Scikit-Learn (a set of very low-resolution handwritten digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TPOTClassifier(\n",
    "    generations=2,\n",
    "    population_size=10,\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    verbosity=0,\n",
    "    config_dict=tpot.config.classifier_config_dict_light,\n",
    "    use_dask=True,\n",
    ")\n",
    "tp.fit(X_train, y_train)\n",
    "\n",
    "# quick look at test-set accuracy\n",
    "\n",
    "sum(tp.predict(X_test) == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only Dask code we explicitly wrote there was a *kwarg* `use_dask=True` \n",
    "\n",
    "Next, let's take a very quick look at one of the Dask datastructures -- a parallel dataframe.\n",
    "\n",
    "We'll look at some records from the Seattle library system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "\n",
    "loans = ddf.read_csv('s3://coiled-training/data/checkouts-small.csv', storage_options={\"anon\": True})\n",
    "\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['Checkouts'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last stop on our quick preview of Dask, let's parallelize some custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def roll_die(sides):\n",
    "    return random.randint(1,sides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local (regular) Python to roll 4d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map(roll_die, [6] * 4)\n",
    "\n",
    "print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our Dask cluster to roll 4d6 in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "roll_die = dask.delayed(roll_die)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_work = map(roll_die, [6] * 4)\n",
    "\n",
    "dask.compute(*cluster_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote (or cloud) clusters vs. local cluster\n",
    "\n",
    "In class, we're using a hosted Coiled cluster. But creating a Dask cluster locally, or on another cluster manager (like Kubernetes) or a public cloud (like AWS) isn't very different.\n",
    "\n",
    "A local cluster (e.g., for working only on your laptop) looks like this\n",
    "\n",
    "```python\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit='2GiB')\n",
    "client = Client(cluster)\n",
    "```\n",
    "\n",
    "One way to start a cluster on Kubernetes looks like\n",
    "\n",
    "```python\n",
    "cluster = KubeCluster.from_yaml('worker-spec.yml')\n",
    "client = Client(cluster)\n",
    "```\n",
    "\n",
    "and one way to start a cluster on AWS Fargate (container service) looks like\n",
    "\n",
    "```python\n",
    "cluster = FargateCluster(image=\"<hub-user>/<repo-name>[:<tag>]\")\n",
    "client = Client(cluster)\n",
    "```\n",
    "\n",
    "We'll look at more options later.\n",
    "\n",
    "## Some more Dask Community resources\n",
    "\n",
    "* Dask issues and source code https://github.com/dask\n",
    "* StackOverflow https://stackoverflow.com/questions/tagged/dask\n",
    "* Dask Github Discussions https://github.com/dask/dask/discussions\n",
    "* Gitter https://gitter.im/dask/dask\n",
    "* Slack\n",
    "  * Dask https://dask.slack.com/\n",
    "  * Coiled Community https://coiled-users.slack.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
