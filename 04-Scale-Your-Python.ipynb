{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing Your Own Python Algorithms\n",
    "\n",
    "<img src='images/city.png' width=600>\n",
    "\n",
    "Although we can do a lot with dataframes and arrays at scale, not every computation we need to do will match these patterns.\n",
    "\n",
    "Dask offers two mechanism for scheduling our own work on the cluster and working with results of distributed and/or long-running computations.\n",
    "\n",
    "* `delayed` which represents a function along with its dependencies, which we may want to run later\n",
    "    * recall that we called `dask.delayed()` on our functions to make them lazy\n",
    "* and `Future` which represents a future result (or failure) from a running, asynchronous function\n",
    "    * we used `client.submit()` to start our example computation, and it returned a Future immediately\n",
    "\n",
    "## Why two systems? Which should we use?\n",
    "\n",
    "There is no single right anwser\n",
    "\n",
    "__`delayed`__ is often more useful when we know the control flow and structure of our computation ahead of time and we want to run a function -- including computing any dependencies -- later.\n",
    "\n",
    "In our toy dataframe example, we'll implement operations `sum` and `count`. Since we know what those computations will look like and require, we'll create `delayed` versions to make them lazy and composable.\n",
    "\n",
    "__`Future`__ makes sense when we want to start a computation immediately, and have complete freedom regarding what will happen to the result: it might be output; it might be a parameter to another function; or it might be a condition that controls what happens next (e.g., if we're running an optimizer until some convergence criterion is reached).\n",
    "\n",
    "*So we'll look at both APIs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed functions and task graphs\n",
    "\n",
    "We can create a delayed, or lazy, function by calling `dask.delayed` or by marking the function definition with the `@delayed` decorator. \n",
    "\n",
    "Most operations on a delayed function (like calls) or object (like property access) are proxied from the delayed to the actual object when needed, but not right away. Until then, if we compose function calls, we are effectively creating a chain (or graph) of delayed objects.\n",
    "\n",
    "When we call `.compute()` or `.persist()`, Dask schedules the necessary functions from the graph and takes care of passing the results to the subsequent functions that rely on them.\n",
    "\n",
    "Before running this code, open up your __Graph__ and __Task Stream__ dashboards -- we'll see some output there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "@delayed\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "my_delayed_square_of_3 = square(3)\n",
    "\n",
    "my_delayed_square_of_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the dependencies involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_delayed_square_of_3.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these diagrams, circles represent delayed functions (tasks), while rectangles represent delayed data items, including the output of any delayed functions.\n",
    "\n",
    "We can compose delayed objects, and evaluate them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_delayed_square_of_the_square_of_3 = square(square(3))\n",
    "\n",
    "my_delayed_square_of_the_square_of_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_delayed_square_of_the_square_of_3.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_delayed_square_of_the_square_of_3.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch computation and persist results for future access, as we did with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted = my_delayed_square_of_3.persist()\n",
    "persisted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your __Graph__ and __Task Stream__ dashboards, notice\n",
    "* Task Stream shows the `square` function we just ran\n",
    "* Graph shows the `square` result resident in cluster memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the value any time with `.compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get us a little closer to our real-world work challenges, let's look at a Monte Carlo estimation of pi. While this estimation is simple, many real-world problems rely on expensive and time-consuming simulation, from Bayesian model sampling to traffic, supply chain, and climate simulations.\n",
    "\n",
    "## Monte Carlo Estimation of π\n",
    "\n",
    "Here, the idea is to sample points from the unit square and count how many of them are also within the unit circle. The proportion of points in the circle to all of our points will approximate pi/4.\n",
    "\n",
    "<img src='images/pi.png' width=300>\n",
    "\n",
    "We'll start with a local version and then try to run it on the cluster with `delayed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample():\n",
    "    x = random.uniform(0, 1)\n",
    "    y = random.uniform(0, 1)\n",
    "    d = x*x + y*y\n",
    "    return 1 if d < 1 else 0\n",
    "\n",
    "sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunch_of_samples(n):\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += sample()\n",
    "    return total/n\n",
    "\n",
    "bunch_of_samples(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pi is about {4* bunch_of_samples(1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a lazy version of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = delayed(sample)\n",
    "\n",
    "output = bunch_of_samples(5)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our output is a delayed, because it depends on a bunch of other delayed outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's make a few improvements. First, take a look in the Task Stream and locate the `sample` tasks. Zoom until you can read how long they take. You should see they take a few microseconds. \n",
    "\n",
    "This is not an optimal task duration because we are paying a high price in scheduler overhead and network communication in order to distribute these tasks.\n",
    "\n",
    "*Dask's scheduler requires several hundred microseconds (say 500-1000 μs) per task* so ideally our tasks should spend significantly longer than this doing their computation.\n",
    "\n",
    "What should we do?\n",
    "\n",
    "Usually:\n",
    "* operate on more data within the task\n",
    "* perform more steps or iterations of an algorithm within the task\n",
    "* or both\n",
    "\n",
    "The obvious choice for our simulation is to run more sampling within each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def multi_sample(n):\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        x = random.uniform(0, 1)\n",
    "        y = random.uniform(0, 1)\n",
    "        d = x*x + y*y\n",
    "        if d < 1:\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples should we run? Let's see how long it takes to run 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sample(1000).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your numbers may vary (depending on hardware, network, etc.) but I'm seeing a very small value. That's similar to scheduler overhead, and we really want our compute to but significantly larger than that. Let's try 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = multi_sample(100_000).compute()\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in the task stream for timing ... this should be better (larger) than the previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits/1e5*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But still not a great estimate of pi. Let's schedule 100 of these (a total of 10 million samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batches(batches, per_batch):    \n",
    "    return sum([multi_sample(per_batch) for i in range(batches)])\n",
    "\n",
    "run_batches(100, 100000).compute() * 4 / 10e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were looking closely, you might have noticed a feature that slipped by in there: `run_batches` returns a delayed because `sum` does ... and that's because the `+` operator is overloaded to work correctly with delayed. \n",
    "\n",
    "Most operations \"just work\" with delayed, but not everything. The breakdown is at https://docs.dask.org/en/latest/delayed-api.html\n",
    "\n",
    "Most critically, we can't make control-flow decisions based on delayeds, because they don't have a truth value (after all, they haven't run at the time of graph/dependency construction).\n",
    "\n",
    "Before moving on, let's look at the __Memory By Key__ dashboard, which will tell us what we are storing in our cluster and how much space it is taking up.\n",
    "\n",
    "Open the Memory By Key dashboard: you should see the result of the `square` operation we `persist`ed earlier, taking up a few bytes.\n",
    "\n",
    "Lets run our sampler and persist the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted_sample_count = run_batches(100, 100000).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that was running, you should see `multi_sample` using the most memory, but when the computation is complete, those intermediate results have been released, and just the final sum (`add` task result) holding a few bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Dataframe: No Magic When You Have `Delayed`\n",
    "\n",
    "Dask Dataframe and other high-level collections can seem magical, but we'll take a look at how Delayed can do a lot of the heavy lifting in an API like that. \n",
    "\n",
    "To illustrate a little bit of how a parallel dataframe can work, as well as give insight into how Dask's low-level constructs can be assembled to create high-level ones, we'll build a toy example.\n",
    "\n",
    "We'll use a small, simple CSV data file, which you can inspect: `df-demo.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "file = 'http://coiled-training.s3.amazonaws.com/data/df-demo.csv'\n",
    "\n",
    "with urllib.request.urlopen(file) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 rows there. Let's imagine that we can only fit 6 rows at a time in memory comfortably, so we want to partition with a partition or blocksize of 6 rows.\n",
    "\n",
    "We can write a function to ingest the data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename, blocksize, partition, schema):\n",
    "    return pd.read_csv(filename, skiprows=partition*blocksize, nrows=blocksize, names=schema, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run it for each partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_big_df(file, blocksize, partition_count, schema):\n",
    "    partitions = []\n",
    "    for i in range(partition_count):\n",
    "        partitions.append(load_csv(file, blocksize, i, schema))\n",
    "    return partitions\n",
    "\n",
    "blocksize = 6\n",
    "partition_count = 3\n",
    "schema = ['player', 'points']\n",
    "\n",
    "myToyDF = load_big_df(file, blocksize, partition_count, schema)\n",
    "\n",
    "for p in myToyDF:\n",
    "    print(str(p) + '\\n' + '-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataframe will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_frame(df):\n",
    "    return pd.concat([p for p in df])\n",
    "\n",
    "whole_frame(myToyDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the replicated index values in our \"big\" dataframe ... you'll see the same behavior with Dask: it's consistent with each partition being its own dataframe with its own index.\n",
    "\n",
    "Notice also that we used a little bit of metadata (number of partitions, schema) that we didn't discover from the file. In a real implementation we'd need to do some inspection of the data to figure out this info, and we'd probably want to store it somewhere! Dask does exactly that when you create a Dask dataframe from a file.\n",
    "\n",
    "For now, we'll \"cheat\" and keep out dataframe as a list of constituent dataframes (partitions)\n",
    "\n",
    "Let's implement two toy operations: count and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_count(df):\n",
    "    return sum(p.shape[0] for p in df)\n",
    "\n",
    "def df_sum(df, col):\n",
    "    return sum(p[col].sum() for p in df)\n",
    "\n",
    "print('Count ', df_count(myToyDF))\n",
    "print('Sum(points)', df_sum(myToyDF, 'points'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a tiny bit of Dask `delayed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.delayed\n",
    "\n",
    "load_csv = dask.delayed(load_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToy_delayed_DF = load_big_df(file, blocksize, partition_count, schema)\n",
    "\n",
    "myToy_delayed_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_frame_from_delayed(df):\n",
    "    return pd.concat([p.compute() for p in df])\n",
    "\n",
    "whole_frame_from_delayed(myToy_delayed_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works ... and check out the Task Stream dashboard: we can see our `load_csv` tasks scheduled in the cluster!\n",
    "\n",
    "Now what about lazy operations on a lazy dataframe ... What about count and sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count(myToy_delayed_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to run -- or at least it doesn't explode -- but it returns a Delayed ... which is actually what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count(myToy_delayed_DF).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why does this work?\n",
    "\n",
    "```python\n",
    "def df_count(df):\n",
    "    return sum(p.shape[0] for p in df)\n",
    "```\n",
    "\n",
    "* the call to `p.shape` accessor and then the subscript `[]` operator on the result are proxied to the underlying object when needed, but return a Delayed in the meantime\n",
    "* the same thing works for the builtin `sum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToy_delayed_DF[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToy_delayed_DF[0].shape.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToy_delayed_DF[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToy_delayed_DF[0].shape[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and so on for most (though not all) operations we might want to do. We can even visualize the dependencies between these lazy objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count(myToy_delayed_DF).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reason, our `df_sum` function will work unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum(myToy_delayed_DF, 'points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum(myToy_delayed_DF, 'points').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as fun as it would be to try and reconstruct more and more of Dask dataframe, that might be a bit much for today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic scheduling with Future\n",
    "\n",
    "Dask's Future interface is like a cluster-aware version of Python's `concurrent.futures` asynchronous programming pattern. If you're not familiar with `concurrent.futures`, that's ok. But for those who are, we'll start with that API and before moving to Dask, where we'll see similar behavior.\n",
    "\n",
    "Let's redefine `multi_sample` as a regular Python function (no `@delayed`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_sample(n):\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        x = random.uniform(0, 1)\n",
    "        y = random.uniform(0, 1)\n",
    "        d = x*x + y*y\n",
    "        if d < 1:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "multi_sample(100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example with Python's `concurrent.futures`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor()\n",
    "out = executor.submit(multi_sample, 100000)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's do that distributed, with Dask__\n",
    "\n",
    "By calling `client.submit()`, we can run this on the cluster and get a handle to the pending result, called a `Future`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_future = client.submit(multi_sample, 100000)\n",
    "sample_future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Task Graph dashboard, you'll notice that the task appears red (since it finishes nearly instantly)\n",
    "\n",
    "If we look at the Future again, it should show *status: finished*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this again more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = client.submit(multi_sample, 100000)\n",
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's a little weird: this time, the Future shows *status: finished* right away, and the result is identical to the earlier sample batch, which seems unlikely.\n",
    "\n",
    "__What's going on?__\n",
    "\n",
    "In order to optimize, Dask has encoded our function arguments into the task data and has assumed our function is __pure__. A pure function is one whose output depends only on the input: no randomness, no side effects.\n",
    "\n",
    "Usually, that's helpful, but in our case we need new random samples every time. We tell Dask that our function is not pure by passing `pure=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = client.submit(multi_sample, 100000, pure=False)\n",
    "sample3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do alot of sampling, we can use a regular Python loop to submit our tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(5):\n",
    "    results.append(client.submit(multi_sample, 100000, pure=False))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, if we're using the same function repeatedly, we can use `client.map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results + client.map(multi_sample, [100000, 110000, 90000], pure=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To block until Futures are finished and collect the output, we call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Managing tasks dynamically: sampling until convergence__\n",
    "\n",
    "We can use futures to schedule sampling tasks continuously until we hit a convergence criterion.\n",
    "\n",
    "Let's start the cluster off with 10 sampling tasks to get started.\n",
    "\n",
    "We can use `as_completed` to access an interator from which can collect results.\n",
    "\n",
    "We'll estimate pi using what we get back, and if our successive estimates aren't close enough to each other, we'll schedule more tasks.\n",
    "\n",
    "> Note: This is just an illustration of dynamic scheduling using Dask `Future` ... in a similar real experiment, since this is a Bernoulli process, the standard error shrinks with ${1/\\sqrt{n}}$ (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval) ... so if we wanted to quantify the likelihood of our estimate, we could just choose a suitable *n* ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "\n",
    "batch_size = 100000\n",
    "\n",
    "initial_future = client.submit(multi_sample, batch_size, pure=False)\n",
    "\n",
    "completion_iterator = as_completed([initial_future])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_estimate = 0.0\n",
    "new_estimate = 1.0\n",
    "estimate_count = 0\n",
    "epsilon = 0.0001\n",
    "\n",
    "for future in completion_iterator:\n",
    "    result = future.result()\n",
    "    \n",
    "    # update estimate: arithmetic moving average\n",
    "    new_estimate = old_estimate * estimate_count + result * 4.0 / batch_size\n",
    "    estimate_count += 1\n",
    "    new_estimate /= estimate_count\n",
    "    print(new_estimate)\n",
    "    \n",
    "    if not abs(new_estimate - old_estimate) < epsilon:\n",
    "        completion_iterator.add(client.submit(multi_sample, batch_size, pure=False))\n",
    "        \n",
    "    old_estimate = new_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Composing Results of Operations with Future__\n",
    "\n",
    "This is handy, so long as we're launching monolithic, embarrassingly parallel tasks on the cluster. \n",
    "\n",
    "But we often need to break our logic up into multiple functions, user other people's code, and combine results in distributed code.\n",
    "\n",
    "`Future` is designed specifically to accommodate composition -- meaning passing the results from one or more function calls into another function. The only catch is ... you can't mix local and remote code transparently, because a lot of local code won't know how to handle Futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2(x):\n",
    "    return 2 + x\n",
    "\n",
    "def make_list(n):\n",
    "    return [None] * (2*n)\n",
    "\n",
    "def write(a_list, index, val):\n",
    "    a_list[index] = val\n",
    "    return a_list\n",
    "\n",
    "def combine_lists(list1, list2):\n",
    "    return list1 + list2\n",
    "\n",
    "# local...\n",
    "\n",
    "combine_lists(write(make_list(add2(1)), add2(0), 7), ['foo', 'bar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with just the first two operations (in evaluation order, from inside out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    make_list(client.submit(add2, 1))\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did this fail? We tried to mix a Future result with a regular local function invocation.\n",
    "\n",
    "We can do this composition if we submit both tasks for async execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = client.submit(add2, 1)\n",
    "r2 = client.submit(make_list, r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can keep going..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = client.submit(write, r2, r1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = client.submit(combine_lists, r3, [-1, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summing up...*\n",
    "\n",
    "We can't supply Futures to arbitrary local functions\n",
    "\n",
    "But we can supply...\n",
    "* regular (non-Future) values to async calls\n",
    "* Futures to async calls\n",
    "* combinations of Futures and regular values when submitting async calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Emergency Services Modeling\n",
    "\n",
    "We'll work on a more complicated simulation-based model to evaluate time-to-response for emergency vehicles in different schemes for Cascadia City.\n",
    "\n",
    "Part of the city is planned as a street grid, and we'd like to look at a few different models where we divide this region into equal-sized zones, and each zone has its own emergency vehicle (which must remain inside that zone).\n",
    "\n",
    "The purpose of our lab is to use Dask to distribute the work, so we'll start with some functions that do most of the calculation work, and focus on running those in the Dask cluster using `Future`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "traffic = np.load('data/traffic.npy') #note this data is local\n",
    "\n",
    "plt.imshow(traffic)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array represents transit time costs (in minutes, under congested conditions) to reach each of the intersections in this 16x16 intersection grid from adjacent intersections.\n",
    "\n",
    "To find travel time between points for the whole grid -- or for a section -- we'll build an *adjacency matrix* and then use a shortest-path algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_chunk_width = 4 # we'll work with square chunks, so N-S and E-W are both 4\n",
    "\n",
    "def build_adj_matrix(costs):\n",
    "    adj_dim = costs.shape[0] ** 2\n",
    "    adj_matix = np.zeros((adj_dim, adj_dim)) # since every pair of locations gets a cost in the adj matrix\n",
    "    \n",
    "    def linear_loc_for_row_col(r, c):\n",
    "        return r + c*costs.shape[0]\n",
    "    \n",
    "    for i in range(costs.shape[0]):\n",
    "        for j in range(costs.shape[1]):\n",
    "            cost_to_ij = costs[i, j]\n",
    "            dest_loc = linear_loc_for_row_col(i, j)\n",
    "            if i > 0:\n",
    "                adj_matix[linear_loc_for_row_col(i-1, j), dest_loc] = cost_to_ij                \n",
    "            if i < costs.shape[0] - 1:\n",
    "                adj_matix[linear_loc_for_row_col(i+1, j), dest_loc] = cost_to_ij                \n",
    "            if j > 0:\n",
    "                adj_matix[linear_loc_for_row_col(i, j-1), dest_loc] = cost_to_ij                \n",
    "            if j < costs.shape[1] - 1:\n",
    "                adj_matix[linear_loc_for_row_col(i, j+1), dest_loc] = cost_to_ij\n",
    "    return adj_matix\n",
    "\n",
    "demo_adj = build_adj_matrix(traffic[0:city_chunk_width, 0:city_chunk_width])\n",
    "plt.imshow(demo_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a helper from `scipy` to find the shortest path (expressed here as travel time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_travel_time_all = shortest_path(demo_adj)\n",
    "plt.imshow(total_travel_time_all)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose there are a fire and a fire truck at particular locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_random_fire(travel_time_matrix, zone_rows, zone_cols):\n",
    "    fire_x = random.randint(0, zone_cols-1)\n",
    "    fire_y = random.randint(0, zone_rows-1)\n",
    "\n",
    "    firetruck_x = random.randint(0, zone_cols-1)\n",
    "    firetruck_y = random.randint(0, zone_rows-1)\n",
    "\n",
    "    travel_from = firetruck_y + zone_rows*firetruck_x\n",
    "    travel_to = fire_y + zone_rows*fire_x\n",
    "    \n",
    "    return travel_time_matrix[travel_from, travel_to]\n",
    "\n",
    "response_sample = response_to_random_fire(total_travel_time_all, city_chunk_width, city_chunk_width)\n",
    "\n",
    "print(\"Travel time\", response_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to measure response time under various scenarios, including ones where more trucks are available.\n",
    "\n",
    "#### Activity 1: Travel time matrices for all zones\n",
    "\n",
    "Divide the full traffic map (matrix) into 16 subsections similar to the one above, and generate travel time matrices for all of them using Dask.\n",
    "\n",
    "Note: in some scenarios we might use Dask array, but for today's exercise, let's use regular NumPy and focus on parallelizing our work with `Future`.\n",
    "\n",
    "Hint: For dividing the matrix into subsections, adapt this sample code using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12], [13,14,15,16]])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = []\n",
    "\n",
    "for outer in map(lambda m : np.vsplit(m, 2), np.hsplit(example, 2)):\n",
    "    for inner in outer:\n",
    "        arrays.append(inner)\n",
    "    \n",
    "arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 2: Emergency response times for all zones\n",
    "\n",
    "Simulate emergency response times for each zone, using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 3: Collect and plot samples for all zones\n",
    "\n",
    "Gather 100 samples for each zone, combine the results, and plot a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 4: Compare zone schemes\n",
    "\n",
    "*Bonus*\n",
    "\n",
    "Simulate\n",
    "* the single-zone model with 16 firetrucks uniformly distributed\n",
    "  * this means 1 zone and `city_chunk_width` of 16\n",
    "  * 16 random firetruck locations, so 16 travel times (choose shortest or mean)\n",
    "* 4-zone model (each zone `city_chunk_width` of 8)\n",
    "\n",
    "Compare the response time distributions to the 16-zone model we've done so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In these activities we didn't focus on making optimal use of the cluster. Take a look at the task stream and see when you were (and weren't) utilizing all of those expensive cores.\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "Take a look at the Dask delayed best practices https://docs.dask.org/en/latest/delayed-best-practices.html\n",
    "\n",
    "There also a set of advanced features under Futures which you are unlikely to often need, but may be useful in solving particularly complex challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
