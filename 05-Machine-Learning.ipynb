{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Dask\n",
    "\n",
    "Seattle has seen an increase in pet registrations over the past few years, and Cascadia City wants to plan registration, animal control, dog parks, and other facilities in line with anticipated growth.\n",
    "\n",
    "We'll analyze a small set of Seattle pet data to try and identify some trends.\n",
    "\n",
    "<img src='images/dog-park.jpg' width=600>\n",
    "\n",
    "Let's spin up a small cluster and dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask ML Overview\n",
    "\n",
    "__The general goal of Dask-style ML is to reproduce, as closely as possible, the Pandas and scikit-learn style workflows that data scientists are used to...__\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_parquet('...')\n",
    "data = df[['age', 'income', 'married']]\n",
    "labels = df['outcome']\n",
    "\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(data, labels)\n",
    "```\n",
    "\n",
    "__... and to integrate smoothly with other ML tools, all while making the necessary changes to support out-of-core (larger than local memory) as well as fully distributed training and scoring use cases.__\n",
    "\n",
    "In some cases, this is a straightforward mission, while for others -- parallelizing algorithms that are traditionally sequential -- it can be trickier.\n",
    "\n",
    "<img src='images/ml.png' width=600>\n",
    "                            \n",
    "Dask's solutions break down into several high-level categories:\n",
    "\n",
    "### Provide Dask ML scalable algorithms\n",
    "\n",
    "Full-scale, out-of-core, distributed processing for several types of tasks, including\n",
    "* Feature engineering (pre-processing)\n",
    "* Linear models / GLMs\n",
    "* Clustering\n",
    "\n",
    "### Out-of-core non-parallel training with scikit-learn\n",
    "\n",
    "For modest datasets that don't require parallelization or a big cluster, but can't fit in memory, Dask provides a wrapper for `Incremental` training of any scikit-learn estimator that supports `partial_fit`, such as Naive Bayes\n",
    "\n",
    "\n",
    "### Parallelize scikit-learn via joblib\n",
    "\n",
    "Scikit-Learn provides limited parallel computing on a single machine via `joblib`. \n",
    "\n",
    "Dask provides a drop-in implementation that extends joblib to many machines in a cluster. This approach works seamlessly any place that scikit-learn supports joblib-based parallelism, but often requires data to fit in memory on each node.\n",
    "\n",
    "Examples include:\n",
    "* Parallelizing random forests\n",
    "* Hyperparameter optimization\n",
    "\n",
    "### Parallel prediction/scoring only: small models, big data\n",
    "\n",
    "Many useful scikit-learn models are small in size, but we want to apply them to large datasets for scoring. Since prediction is usually an embarrassingly parallel task, Dask provides a wrapper that scales `model.predict` to large data and large clusters.\n",
    "\n",
    "### Partner with other distributed libraries\n",
    "\n",
    "Some machine learning libraries like XGBoost already have distributed solutions that work well. Dask-ML makes no attempt to re-implement these systems. Instead, Dask-ML makes it easy to use normal Dask workflows to prepare and set up data, then it deploys XGBoost automatically *alongside* Dask, and hands the data over. The result is a smooth API using best-of-breed implementations.\n",
    "\n",
    "## Dask ML and total pet registrations\n",
    "\n",
    "We'll start off trying a simple model to measure the growth in total pet licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "\n",
    "pets = ddf.read_csv('s3://coiled-training/data/pets.csv', parse_dates=[\"License Issue Date\"], \n",
    "                    blocksize=1e6, \n",
    "                    dtype={'License Number': 'object',\n",
    "                           'ZIP Code': 'object'},\n",
    "                    storage_options={\"anon\": True})\n",
    "pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like if we drop the \"Secondary Breed\" column, we'll have a lot more complete cases.\n",
    "\n",
    "Let's do that, drop \"License Number\" as well, filter out incomplete records, and simplify the column names a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = pets.drop(columns=['Secondary Breed', 'License Number' ]).dropna()\n",
    "\n",
    "pets = pets.rename(columns={'License Issue Date':'license_date','Animal\\'s Name':'name',\n",
    "                            'Species':'species', 'Primary Breed':'breed', 'ZIP Code':'zip'})\n",
    "\n",
    "pets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be handy to have the day represented as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pets['day'] = pets['license_date'].apply(pd.Timestamp.toordinal)\n",
    "pets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first goal just to count the registrations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_counts = pets['day'].value_counts()\n",
    "pet_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the dataset isn't really big ... but let's imagine it might be, and downsample to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_counts_local_sample = pet_counts.sample(0.5).compute()\n",
    "pet_counts_local_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_array = pet_counts_local_sample.reset_index()\n",
    "local_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_array.columns = ['day', 'registrations']\n",
    "\n",
    "local_array.plot('day', 'registrations', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very linear! Maybe we should take the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(local_array.day, np.log(local_array.registrations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still doesn't look great, but it's a little more promising.\n",
    "\n",
    "Remember this is the local, downsampled data.\n",
    "\n",
    "So our gameplan for Dask is to \n",
    "* apply these transforms on the full dataset\n",
    "* perform a train/test split\n",
    "* model\n",
    "* evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_counts = pet_counts.reset_index()\n",
    "pet_counts.columns = ['day', 'registrations']\n",
    "pet_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "pet_counts['log_registrations'] = da.log(pet_counts.registrations)\n",
    "pet_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert to Dask array and measure the chunk sizes (otherwise, Dask may not know how many records we have in each chunk, since reading a dataframe or series is a lazy operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pet_counts['day'].to_dask_array(lengths=True)\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pet_counts['log_registrations'].to_dask_array(lengths=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll prepare our training/test split\n",
    "\n",
    "Notice\n",
    "* we import `train_test_split` from `dask_ml.model_selection`\n",
    "    * it works like the scikit-learn version but supports parallel operation on large, distributed data\n",
    "* since we only have 1 predictor, but the function expects a 2-axis array (feature matrix), we reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictor.reshape(-1, 1), response, test_size=0.1)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a model. \n",
    "\n",
    "* The syntax pattern (instantiate, configure, fit) looks a lot like scikit-learn, \n",
    "* but we're importing from `dask_ml.linear_model` and were configuring a distributed, iterative solver.\n",
    "\n",
    "This one is a second-order (quasi-Newton) solver that's popular for modest numbers of predictors (narrow matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(solver='lbfgs', max_iter=5)\n",
    "lr_model = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `model.predict` step is distributed as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = lr_model.predict(X_test)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our measurement step -- here we're looking at RMSE on log scale -- looks like scikit-learn but comes from the parallel implementation in `dask_ml.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_predicted))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(y_train.std().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably not a stellar result for the city! \n",
    "\n",
    "* We're off by around a factor of 2.5 (vs. a mean-baseline that would be off by 3.8x)\n",
    "\n",
    "But the goal here was really to explore the API and method -- luckily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus activity idea: Poisson regression*\n",
    "* Since we're modeling count data over time in this task, Poisson regression might be a better fit.\n",
    "* Trying that out with Dask's GLMs is a very small change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask, scikit, joblib, and counting cats\n",
    "\n",
    "<img src='images/cat.jpg' width=600>\n",
    "\n",
    "Pet trends seem to come and go in different Seattle neighborhoods, so it seems reasonable and useful to try and forecast registration of dogs and cats by zip code. \n",
    "\n",
    "For this task, we'll look at two more Dask features\n",
    "* Categorical data for the zip code feature and pet species\n",
    "* Scaling a random forest with Dask and joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets.species.value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_and_dogs = pets[(pets['species'] == 'Dog') | (pets['species'] == 'Cat')]\n",
    "cats_and_dogs = cats_and_dogs[['day', 'zip' , 'species']]\n",
    "cats_and_dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use scikit-learn's `Pipeline` class and `make_pipeline` helper with Dask components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from dask_ml.preprocessing import Categorizer, LabelEncoder, DummyEncoder\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    Categorizer(),\n",
    "    DummyEncoder()\n",
    ")\n",
    "\n",
    "pipe.fit_transform(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works ... and the code looks nice, simple and familiar. But it leads us toward an unnecessary high-dimensionality problem, especially since we're going to be using a forest algorithm. \n",
    "\n",
    "Let's label encode the categorical instead of one-hot encoding it. We can use a `LabelEncoder` for the zip predictor as well as the categorical (species) response.\n",
    "\n",
    "Note that\n",
    "* We need a parallel-aware LabelEncoder, because Dask needs to identify the union of all values for that field, and then assign them uniform labeling throughout the dataset\n",
    "* In fact, the same requirement applied to the one-hot encoding above (uniform label map prior to generating the one-hot columns)\n",
    "* `LabelEncoder` supports a slightly non-standard API within scikit-learn, making it hard to use with a `Pipeline` in Dask (or scikit-learn)\n",
    "    * for more standard transformations, Dask *does* have a version of scikit-learn's `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_and_dogs = cats_and_dogs.categorize()\n",
    "cats_and_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_and_dogs['zip'] = LabelEncoder().fit_transform(cats_and_dogs['zip'])\n",
    "cats_and_dogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_and_dogs['species'] = LabelEncoder().fit_transform(cats_and_dogs['species'])\n",
    "cats_and_dogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cats_and_dogs[['day', 'zip']], \n",
    "                                                    cats_and_dogs.species, test_size=0.1)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of options for measuring our performance.\n",
    "\n",
    "In the case where we have small data and can train a sklearn model locally (or load a model trained elsewhere), we can use Dask to parallelize post-fit operations like `transform`, `predict`, and `predict_proba`.\n",
    "\n",
    "Dask's `ParallelPostFit` wrapper/meta-estimator can make predictions using parallel tasks for *any* sklearn estimator because, under the hood, it's basically just doing a `map_partitions` or `map_blocks` with the relevant function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we launch this, let's think a moment about what's going on.\n",
    "* We've got a local scikit-learn model that may be large\n",
    "* We want to use it to score records on remote workers\n",
    "* That means the model needs to get to all of those workers\n",
    "\n",
    "If we have high bandwidth between our nodes *and* we get a really valuable speedup from parallel prediction at scale, then this makes sense. But keep in mind it may not always pay off.\n",
    "\n",
    "Consider the size of our random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "p = pickle.dumps(rf)\n",
    "print(sys.getsizeof(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't mind shipping this around, we are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "parallel_predicting_scorer = ParallelPostFit(estimator=rf, scoring='accuracy')\n",
    "\n",
    "predictions = parallel_predicting_scorer.predict(X_test)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's no better than random since dogs outnumber cats in our dataset 2:1 ... The mayor is just about ready to show us the door and invite some better data scientists to join the team.\n",
    "\n",
    "Luckily, we have a job waiting for us on the engineering and ops side.\n",
    "\n",
    "But before we head out for the day, let's try a lab exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Dask + XGBoost\n",
    "\n",
    "### Activity 1: Train the cat-dog data with XGBoost\n",
    "\n",
    "As of 2020, XGBoost has a new, official API for working with Dask (although the older `dask-xgboost` package still works).\n",
    "\n",
    "The new API is documented at https://xgboost.readthedocs.io/en/latest/tutorials/dask.html\n",
    "\n",
    "First, we'll just train a model. Most of the configuration information in this API is passed via a parameters object, described here: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "To get started, keep it as simple as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2: Predict on the test set\n",
    "\n",
    "We'll need to make a DMatrix again to feed the test set to XGBoost. \n",
    "\n",
    "XGB also has a Dask-specific API for distributed prediction.\n",
    "\n",
    "See if you can generate a vector of predictions and inspect those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3: Accuracy\n",
    "\n",
    "Using a distributed mechanism, convert the prediction probabilities into an accuracy score for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab wrapup notes\n",
    "\n",
    "XGBoost does still implement scikit-learn-style interfaces, such as `DaskXGBClassifier` and `DaskXGBRegressor` ... and there's also the scikit-style `dask-xgboost` package.\n",
    "\n",
    "If the goal of Dask ML is to provide an experience like scikit, and these scikit-style wrappers exist for XGBoost, why did we look at an API that feels different?\n",
    "\n",
    "* We've done scikit-style examples, so seeing more of the same isn't as valuable\n",
    "    * i.e., you probably already know how to use those other APIs if you need to\n",
    "* Any time a tool presents a new *official* API, it's usually a good idea to try it out and consider a path forward, should the other APIs become deprecated or unsupported\n",
    "* XGBoost is such a powerful and popular tool -- running across many platforms -- that it may be valuable to use its standard API, such as the `params`-based configuration\n",
    "* Of course, at the end of they day, it's your project and your decision: some scikit-style examples are at https://github.com/dmlc/xgboost/blob/master/demo/dask/sklearn_cpu_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up a whirlwind tour\n",
    "\n",
    "This has been a brief introduction to core workflows in Dask machine learning, and there are certainly plenty of areas we haven't explored in this introduction!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
