{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number Crunching with Dask Array\n",
    "\n",
    "We'll be helping the zoning and land-use departments choose locations for new recreation facilities in town. For that, we'll need to analyze array data reflecting the local topography.\n",
    "\n",
    "This will involve\n",
    "* understanding array data\n",
    "* loading relevant datasets with Dask array\n",
    "* analyzing the data to locate key sites\n",
    "* writing the processes data to a suitable format\n",
    "\n",
    "## What is array data?\n",
    "\n",
    "If you're a scientist or data analyst who works with array data extensively, this part won't be too surprising. But for folks who typically work with tabular data, a brief intro to array data will get us ready for Dask Array\n",
    "\n",
    "__Array data__ typically refers to a rectangular, multidimensional collection of values, all of the same type.\n",
    "\n",
    "Multidimensional is usually, though not always, small (say, 2-5) and the data type is usually, though not always, numeric\n",
    "\n",
    "In Python, we typically use the NumPy library to work with this sort of data, and the array data structure is called NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(0, 1, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underneath the hood, a lot of tabular data is array data\n",
    "\n",
    "For example, we might start with a Pandas data table and featurize it for machine learning, producing a table of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'league':['B', 'B', 'A', 'C'], 'games':[34,22,66,12]})\n",
    "\n",
    "prepared = pd.get_dummies(df)\n",
    "\n",
    "prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.values.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, where our data consist completely of numeric measurements, we go straight to the array representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features have specific, meaningful names, but often we have data where that isn't the case.\n",
    "\n",
    "For example, the pixels in this image don't have meaningful names; they make more sense as an array\n",
    "\n",
    "<img src='images/trees.png'>\n",
    "\n",
    "We can load and manipulate them as array data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imagedata = imageio.imread('images/trees.png')\n",
    "\n",
    "print(type(imagedata), imagedata.dtype, imagedata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imagedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an alpha channel ... does it have any data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imagedata[:,:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the alpha channel and convert to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_alpha = imagedata[:,:,:3]\n",
    "\n",
    "no_alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale = no_alpha.mean(axis=2)\n",
    "\n",
    "grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "\n",
    "plt.imshow(grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we started with 3 axes, sliced to discard the alpha channel, then averaged over the color axis to get approximate intensity, and ended with a grayscale matrix.\n",
    "\n",
    "It's easy to imagine large datasets where we might want to do similar operations\n",
    "* 10 minutes of 4K video at 60Hz might look like an array of shape (2160, 3840, 3, 36000) and consume almost 1TB uncompressed\n",
    "* Even at low resolution, global satellite imagery data, atmospheric data, ocean data, and similar datasets will likely be big\n",
    "\n",
    "## Ok, now let's bring Dask into the discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Array data at rest__\n",
    "\n",
    "In some domains, we may come across array data stored as, say, CSV files or pickled Torch Tensors, but often we'll work with dedicated formats specifically designed around array datasets.\n",
    "\n",
    "One popular format is HDF5\n",
    "* https://www.neonscience.org/about-hdf5\n",
    "* https://en.wikipedia.org/wiki/Hierarchical_Data_Format\n",
    "\n",
    "Dask array can consume these in a lazy, parallel manner ... it would look like this:\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "import dask.array as da\n",
    "\n",
    "file = h5py.File('datafile.h5', 'r')\n",
    "dataset = file['dataset_within_file']\n",
    "arr = da.from_array(dataset)\n",
    "```\n",
    "\n",
    "The challenge comes when working with cloud data storage. The Python integrations to cloud storage a bit quirky or problematic when processing HDF5. For some datasets, it might work, and would look like:\n",
    "\n",
    "```python\n",
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem() # credentials should be in env before this point\n",
    "file = h5py.File(s3.open('s3://bucket/path/data.h5', 'rb'))\n",
    "\n",
    "dataset = file['dataset_within_file']\n",
    "arr = da.from_array(dataset)\n",
    "```\n",
    "\n",
    "To avoid those quirks/issues, though, we'll work with a more modern and efficient format, in terms of both compression and usefulness for parallel computation, called Zarr\n",
    "* https://zarr.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "arr = da.from_zarr('s3://coiled-training/data/land', storage_options={\"anon\": True})\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seemed to work ... and we get a nice HTML view of our 3-axis dataset.\n",
    "\n",
    "But there are a couple of problems. \n",
    "\n",
    "First, this seems like at least an axis too many: after all, how many elevations can a single coordinate pair on earth have? We'll come back to that part -- how to decode the data. It's not uncommon to need to decode datasets in a situation like this.\n",
    "\n",
    "Next, and more severe: if we try to operate on this dataset right now, \n",
    "* the whole thing will end up getting loaded in one worker (if it fits)\n",
    "* we won't get any parallelism\n",
    "\n",
    "*Unlike Dask dataframe, Dask array does not automatically create partitions*\n",
    "\n",
    "We need to give it some hints. Dask array partitions are called *chunks* or *blocks* and they're a little different from Dask dataframe partitions.\n",
    "\n",
    "Here's what a Dask array looks like, conceptually\n",
    "\n",
    "<img src='images/dask-array.svg' width=600>\n",
    "\n",
    "A Dask array is composed of multiple \"regular\" NumPy NDArray chunks... but they can be divided into pieces along all of the axes.\n",
    "\n",
    "From the documented best practices, \n",
    "* You want to choose a chunk size that is large in order to reduce the number of chunks that Dask has to think about (which affects overhead)\n",
    "* but also small enough so that many of them can fit in memory at once.\n",
    "* Dask will often have as many chunks in memory as twice the number of active threads.\n",
    "\n",
    "In a real application, this typically means chunks of 100MB or even much more, but for our exercise we'll have smaller data and much smaller chunks.\n",
    "\n",
    "Let's try dividing on the first two axes (roughly speaking, latitude and longitude) but not the third (\"depth\") axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = da.from_zarr('s3://coiled-training/data/land', chunks=(200, 200, 4), storage_options={\"anon\": True})\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see stats for the overall dataset under the Array column, while stats for each block appear under the Chunk column.\n",
    "\n",
    "The visualization is helpful as well, although in high numbers of dimensions (axes) that might prove less useful.\n",
    "\n",
    "The blocks are indexed along all axes, and we can access them (if needed) in a similar way to the partitions of Dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.blocks[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr.blocks[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.blocks[0,0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr.blocks[0,0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Dask arrays also have a `chunks` property, which is a different: `chunks` provides tuples of sizes for the individual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems kind of silly at first, because we told Dask how to chunk the array in the first place, so shouldn't these sizes match?\n",
    "\n",
    "Not necessarily. In our example, the chunks divide the dataset evenly. But that doesn't have to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uneven = da.random.uniform(0, 10, size=(17, 23, 29), chunks=(5,7,11))\n",
    "uneven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uneven.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the chunk dimensions don't evenly divide the dataset. While not ideal, this may make sense in a lot of situations. Perhaps the main task we're performing for our work naturally occurs on blocks of size 17x23x29. In that case, the uneven chunking may allow more efficiency overall.\n",
    "\n",
    "The critical point is that \n",
    "* the `blocks` property exposes delayed references to actual blocks of data; \n",
    "* the `chunks` property exposes the actual chunk dimensions along each axis, while \n",
    "* the `chunksize` property stores the requested or target size of a chunk,\n",
    "    * even if not every chunk matches this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uneven.chunksize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "The Dask array API exposes most, though not all, of the NumPy API. So we can use our NumPy knowledge to apply operations to our data.\n",
    "\n",
    "In the land elevation dataset, the data are actually image tiles, where the fourth axis encodes color and alpha information.\n",
    "\n",
    "The alpha channel should be uninformative... let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, 3].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, 3].min().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, 3].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask collections -- like Array and Dataframe -- \"hide\" their task graph information in a member called `dask`\n",
    "\n",
    "Until recently, inspecting the Dask graph was tedious and difficult, since it included a very large number of tasks. Recall that a Dask task is just a Python function, so it's often just one step of a calculation on just one chunk of a data structure -- even one calculation, if it's to be performed over 1,000 blocks, would mean 1,000 tasks will be in the full graph.\n",
    "\n",
    "Recently, a series of under-the-hood improvements to Dask performance have also made it easier to inspect Dask computations by exposing the *high-level graph*. Each item in the high-level graph is typically a single calculation -- but applied to all of the blocks, not just one. That means high-level graphs are much smaller, making them more friendly for the Dask `Client` to work with, and for humans.\n",
    "\n",
    "We can inspect the high-level graph corresponding to that last computation like this. (If you like seeing what Dask plans to do with your data, you can back and take a look at `.dask` on some of our dataframe examples as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, 3].max().dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level graph visualization gives a static view of the planned work. But we can also see a dynamic, \"live\" view that corresponds to this coarse-grained execution by opening the __Groups__ (short for \"Task Groups Graph\") dashboard. Open that up and run the calculation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, 3].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a bit quick -- but for your longer, larger data processing work, you'll have time to inspect the graph and observe the progress animation to see how it is going.\n",
    "\n",
    "For now, to \"freeze\" the graph, we can cache our Dask array following the same API pattern we used for caching dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_example = arr[:, :, 3].max().persist() # graph should remain in the Groups dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for our topography data, we've learned two things: indeed, the alpha channel has no signal for us. We can get rid of it.\n",
    "\n",
    "More importantly, Dask array follows the same convention we've seen with dataframe and our own delayed objects: they are lazy by default, and we can use `.compute` when we want a result that is \n",
    "* a regular Python object\n",
    "* completely loaded in our local Python process\n",
    "\n",
    "As with dataframe, when we are producing a large-scale result, we probably want to either keep a handle (delayed) that we can perform more operations on, or, if we're finished, write it out in parallel using the cluster.\n",
    "\n",
    "Let's keep just the RGB image channels and write it out -- note that Zarr format preserves our partitioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.environ['WRITE_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:, :, :3].to_zarr('s3://' + bucket + '/elevations', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load this data, the structure is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevations = da.from_zarr('s3://' + bucket + '/elevations')\n",
    "elevations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask array documentation is at https://docs.dask.org/en/latest/array.html\n",
    "\n",
    "Let's run a couple of more expensive computations -- just toy calculations for now -- and introduce two new dashboards.\n",
    "\n",
    "__Aggregate Time Per Action__ dashboard summarizes the time spent on categories of operations, like *compute*, *data transfer*, and *serialization* rather than at the level of individual tasks. For a high level performance view, this can be a better place to start than looking at time spend on specific tasks.\n",
    "\n",
    "With that new dashboard open as well as the Task Stream, let's do some matrix multiplication and then add all of the resulting values.\n",
    "\n",
    "Keep an eye on both dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(elevations[:, :, 0] @ elevations[:, :, 1].transpose()).sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo illustrates how the aggregate time report summarizes a lot of lower-level complexity in the task stream (as well as doing the arithmetic on total time for you).\n",
    "\n",
    "Open the __Compute Time Per Key__ dashboard, which presents the expensive tasks summarized in a bar or pie chart. You should easily see that the `matmul-sum` task used the biggest chunk of time. That information -- and the color coding of the task bar itself -- should match what you're seeing in the Task Stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to do some urban planning\n",
    "\n",
    "Let's try and decode the elevation grid.\n",
    "\n",
    "First we'll do the quick-and-dirty recipe (we'll save the precise calculation for our lab exercises).\n",
    "\n",
    "The pixel instensity at a point corresponds to the relative elevation, scaled between 0 and (approximately) 1000 feet above sea level.\n",
    "\n",
    "You can almost see the valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(elevations.blocks[0,0].compute()[100:, 100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with NumPy, we can average across the depth (RGB) axis to get a 2-d map of elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_map = elevations.mean(axis=2)\n",
    "elevation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale to get feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_elevation_map = 1000.0 * elevation_map / 255\n",
    "scaled_elevation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the area with the least rugged terrain for some campuses, office parks, and recreational spaces.\n",
    "\n",
    "Let's start by looking for minimal variance within a block.\n",
    "\n",
    "Dask array has a convenient API for running an operation for every block. Since we may want to transform the existing block to some new value, it's called `map_blocks` and takes a transforming function. In our case, we can use NumPy's `std` \n",
    "\n",
    "... but there's one small gotcha with `map_blocks`: it expects the output from the mapping (transforming) function to be the same shape (axes/size) as the input.\n",
    "\n",
    "So we'll dress up our scalar output as a NDArray of shape (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elevation_std(array_block):\n",
    "    return np.array([[np.std(array_block)]])\n",
    "\n",
    "variance_da = scaled_elevation_map.map_blocks(elevation_std, chunks=(1,1))\n",
    "\n",
    "variance_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = variance_da.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can visually inspect to see where the terrain might be smoothest ... but if we had a bigger dataset we might need to find that block programmatically or by visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly see that block 2,5 is worth a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_block = scaled_elevation_map.blocks[2,5].compute()\n",
    "\n",
    "print(target_block.min(), target_block.mean(), target_block.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This areas looks promising so let's start from the top, improve our work so far, and then go a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Investigating topography\n",
    "\n",
    "#### Activity 1: Proper decoding of image data\n",
    "\n",
    "Since human eyes have different sensitivities to red, green, and blue, a grayscale image is usually not encoded in RGB format using the same intensities (pixel values) for all 3 channels. In fact, even averaging across the channels doesn't always get you the right grayscale level.\n",
    "\n",
    "This terrain data was encoded using the \"luminosity\" method, which means our real values are determined by this formula: 0.21 R + 0.72 G + 0.07 B\n",
    "\n",
    "Use that formula to convert the 3-channel data to an elevation map. Then scale the elevations: \n",
    "* reference values for min and max elevation for this map area are 30 feet (min) to 1100 feet (max)\n",
    "* because of how the maps are generated, those reference elevations may not actually exist in this area\n",
    "\n",
    "Let's transform and then find the min/max in our land area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 2: Finding least-rugged terrain block\n",
    "\n",
    "Let's repeat our elevation variance investigation. See if you can also determine the coordinates of the \"least-variance\" block programmatically using Dask (not NumPy).\n",
    "\n",
    "Hint: if we want Dask to know the shape of our array after calling `map_blocks`, we have to give that API call some info by using the `chunks=` parameter (since otherwise it has no idea of the shape our `elevation_std` will return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 3: Zoom in for smoother land\n",
    "\n",
    "*Bonus Project*\n",
    "\n",
    "Now that we know roughly where to search, let's zoom in on that specific block in the elevation map. It's 200x200 units, and those units are pretty big. Let's further divide that block into 10x10 areas and look again. Hint: find the original block and `rechunk`\n",
    "\n",
    "For more information on how the elevation changes, try calculating `min` or `max` for the chunks and then generating a report of changes with `diff`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab Postscript\n",
    "\n",
    "Another way to investigate these sorts of questions would be to perform some analysis for every group of elements -- e.g., a point and its neighborhood -- even when those points are on the edge of a block. \n",
    "\n",
    "Dask array has an API, `map_overlap`, that allows us to operate over a block but also have access to a small group of neighboring values that adjoin the block edges. Using this facility, we can do things like perform convolution/cross-correlations, create sliding (windowed) calculations like moving averages, approximate gradients ... which might be really helpful in finding smooth land!\n",
    "\n",
    "More details at https://docs.dask.org/en/latest/array-overlap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "As with Dask dataframe, you can cache results using `persist`\n",
    "\n",
    "Some additional best practices are at https://docs.dask.org/en/latest/array-best-practices.html\n",
    "\n",
    "Both Zarr and HDF5 are solid formats for array data; avoid formats that don't align with the scalable reading and writing of arrays, like text-based or columnar formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swappable Implementations\n",
    "\n",
    "Much of Dask array can be applied to alternative implementations of the NumPy interface\n",
    "\n",
    "Among such implementations, the GPU-backed CuPy arrays (originally part of the Chainer project) are of particular interest: https://docs.dask.org/en/latest/gpu.html\n",
    "\n",
    "Additional, even newer, implementations may be usable in the future, including\n",
    "* JAX - jax.numpy https://jax.readthedocs.io/en/latest/jax.numpy.html\n",
    "* TensorFlow - tf.experimental.numpy https://www.tensorflow.org/api_docs/python/tf/experimental/numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
