{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapup\n",
    "\n",
    "### Best-practices link roundup\n",
    "\n",
    "The Dask docs collect a number of best practices:\n",
    "\n",
    "-   Dataframe: <https://docs.dask.org/en/latest/dataframe-best-practices.html>\n",
    "-   Array: <https://docs.dask.org/en/latest/array-best-practices.html>\n",
    "-   Delayed: <https://docs.dask.org/en/latest/delayed-best-practices.html>\n",
    "-   Overall: <https://docs.dask.org/en/latest/best-practices.html>\n",
    "\n",
    "### Partitions/Chunks and Tasks\n",
    "\n",
    "Remember that Dask is a scheduler for regular Python functions operating on (and producing) regular Python objects.\n",
    "\n",
    "Your partitions, chunks, or data segments should be small enough to comfortably fit in RAM for each worker thread/core.\n",
    "\n",
    "At the limit, that means...\n",
    "\n",
    "-   if you have a 1GB worker with 1 core, want to keep your partitions below 1GB\n",
    "-   with 2 x 1 GB workers with 1 cores, we still want partitions below 1GB\n",
    "-   with n x 4 GB workers with 2 cores per worker, we want partitions below 2 GB\n",
    "\n",
    "But...\n",
    "\n",
    "- Some tools, like Pandas, may require significant overhead to process a partition. In that case, you may want partitions that can fit 2x-4x in RAM\n",
    "- It's also helpful to have a few chunks of data available to keep Dask's worker cores busy.\n",
    "\n",
    "So we might want to take those numbers above and make them 2-4x smaller (or, equivalently, create 2-4x as many partitions).\n",
    "\n",
    "Generally speaking, a lot of tasks is not a bad thing...\n",
    "* Scheduling overhead for each additional task is typically less than 1 millisecond, and can be a lot less\n",
    "* That said, if you have, say, a million tasks, those milliseconds will add up to minutes. In that case you may want to simplify your task graph or use larger (and hence fewer) partitions/chunks.\n",
    "* We want to have significant work for each task relative to scheduler cost, so hundreds or even thousands of milliseconds is ok for task compute time\n",
    "\n",
    "### Caching (Persistence)\n",
    "\n",
    "The results of computations can be cached in the cluster memory, so that they are available for reuse, or for use to derive subsequent results.\n",
    "\n",
    "(See: `persist` which is available on `Client`, `Bag`, `Array`, `Dataframe`, etc.; `Future` results are cached by default)\n",
    "\n",
    "Use caching wisely (not indiscriminately) and monitor memory usage using the dashboard.\n",
    "\n",
    "### Data \n",
    "\n",
    "__Location__\n",
    "\n",
    "Choose data locations...\n",
    "* to minimize __(amount of data)\\*(network cost of moving the data)\\*(how often that data needs to move)__\n",
    "* there may not be a perfect arrangement, especially across different workloads\n",
    "* decompression cost is usually smaller than the cost of moving uncompressed data\n",
    "\n",
    "__Formats and Compression__\n",
    "\n",
    "Use compression schemes which are *splittable* and allow random access, so that processing your files in parallel is more flexible, e.g., Snappy, LZ4 instead of gzip.\n",
    "\n",
    "For datasets, consider files (and collections of files) in Parquet, ORC, HDF5, etc.\n",
    "\n",
    "\n",
    "### Dask project documentation quick links\n",
    "* Main project page https://dask.org/\n",
    "* Core documentation https://docs.dask.org/en/latest/\n",
    "* Distributed (scheduler) https://distributed.dask.org/en/latest/\n",
    "* Machine learning https://ml.dask.org/\n",
    "* Deployment tools\n",
    "    * Kubernetes https://kubernetes.dask.org/en/latest/\n",
    "    * AWS or Azure https://cloudprovider.dask.org/en/latest/\n",
    "    * YARN https://yarn.dask.org/en/latest/\n",
    "\n",
    "### Community resources\n",
    "* Dask issues and source code https://github.com/dask\n",
    "* Dask Github Discussions https://github.com/dask/dask/discussions\n",
    "* StackOverflow https://stackoverflow.com/questions/tagged/dask\n",
    "* Gitter https://gitter.im/dask/dask\n",
    "\n",
    "### Coiled Computing\n",
    "* https://coiled.io/\n",
    "* Coiled Cloud https://coiled.io/cloud/\n",
    "* Join the Coiled Slack community \n",
    "\n",
    "# Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
