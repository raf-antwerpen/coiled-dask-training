{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL and Analytics with Dask Dataframe\n",
    "\n",
    "The mayor is convinced that Cascadia City's library usage patterns and trends will resemble those of Seattle. So we've been asked to take a dataset from Seattle and prepare it for a number of reporting tasks. \n",
    "\n",
    "This will involve\n",
    "* consuming the raw Seattle library data, which is in a plaintext format\n",
    "* selecting specific columns and rows which are important to Cascadia City, while discarding others\n",
    "* writing the refined dataset out in a more efficient binary format (Apache Parquet)\n",
    "* generating various reports on usage\n",
    "\n",
    "Along the way, we need to make sure we understand Dask dataframe well enough to help out the library administration and other departments.\n",
    "\n",
    "<img src='images/library.jpg' width=600>\n",
    "\n",
    "We'll start out by getting access to our Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use Dask dataframe to access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "\n",
    "loans = ddf.read_csv('s3://coiled-training/data/checkouts-small.csv', storage_options={\"anon\": True})\n",
    "\n",
    "loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This looks a bit different from a Pandas dataframe ... so:__\n",
    "\n",
    "## What *is* a Dask dataframe?\n",
    "\n",
    "A Dask dataframe is a collection of Pandas dataframes, divided along the index. You can picture it like this:\n",
    "\n",
    "<img src='images/dask-dataframe.svg' width=400>\n",
    "\n",
    "The smaller Pandas dataframes which make up the larger, virtual Dask dataframe, are called *partitions*\n",
    "\n",
    "So, at the top of the following output, the label __npartitions=__ refers to the number of constituent Pandas dataframes. You'll notice that Dask automatically chose a number of partitions to use, although you can customize that if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You won't often need to interact with individual partitions, but you can if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.partitions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait ... I thought you said the partition was a __Pandas__ dataframe!\n",
    "\n",
    "It is ... but we haven't computed it yet.\n",
    "\n",
    "In order to minimize extra computation, data movement, memory, and time, Dask's data structures try to be *lazy*\n",
    "\n",
    "This allows them to optimize their operation: for example, maybe you end up needing just 2 columns out of a 900-column-wide table ... in that case, it makes sense to see what's really needed before loading all of the data\n",
    "\n",
    "When we want to materialize a local, Python object, we add `.compute()` to API call\n",
    "\n",
    "So, to tell Dask that we want to load up that partition locally, we could type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.partitions[2].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a lot like Pandas output, but we can check to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loans.partitions[2].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to see a few records, we don't need to load even a single partition, though. Dask will give a preview with the `.head()` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, wait ... I thought you just said I needed `.compute()`!\n",
    "\n",
    "#### __When do I need `.compute()` and how can I tell?__\n",
    "\n",
    "__Do call__ `.compute()` when you want a full Pandas object -- Dataframe or Series -- calculated for you *and* you want it loaded up in your local Python process (where your `Client` object lives).\n",
    "\n",
    "This is typical for small, report-type outputs, like we'll do later in this notebook.\n",
    "\n",
    "__Don't__ call `.compute()` on a huge Dask dataframe, because it likely won't fit in local memory anyway\n",
    "\n",
    "__Don't__ call `.compute()` if the goal is to write out a large dataset (perhaps one that you've transformed) to disk. There are APIs for doing that directly from the cluster, in parallel, so that your local process doesn't have to deal with all that data.\n",
    "\n",
    "__Don't__ call `.compute()` if there are simpler APIs designed for human, interactive consumption that might be more efficient, like `.head()` or `len()`\n",
    "\n",
    "E.g., if we want to count the total number of rows in our dataframe, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can I do with Dask dataframe and how do I do it?\n",
    "\n",
    "If you're used to Pandas, it takes a little adjustment to get used to working with data without seeing all those nice rows and columns on the screen. But most of the operations you're used to -- selecting and transforming columns, filtering rows, grouping and aggregating -- still work.\n",
    "\n",
    "In our first library task, we need to throw out the __Publisher__ and old row number (\"__Unnamed: 0__\") column as well as \"old\" data.\n",
    "\n",
    "First, let's drop the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans2 = loans.drop(columns=['Publisher', 'Unnamed: 0'])\n",
    "loans2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is \"old\" data? Let's find all of the years in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans2['CheckoutYear'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... this is a \"lazy\" Dask Series, but we really want the actual, concrete Series of unique years.\n",
    "We know this will be a small collection, so __it's time for `.compute()`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.CheckoutYear.unique().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking with the bosses, it looks like we only want records from 2010 onward, and we want to omit the 2020 data since it's ... anomalous. So we can filter that whole dataset using a Pandas-style filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans3 = loans2.query('CheckoutYear >= 2010 & CheckoutYear < 2020')\n",
    "\n",
    "#NOTE: loans2[(loans2['CheckoutYear'] >= 2010) & (loans2['CheckoutYear'] < 2020)] would work but is less efficient\n",
    "\n",
    "loans3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we've been asked to *drop all incomplete records* and then write out the cleaned, post-2010 dataset in Apache Parquet format, *partitioned by CheckoutYear*.\n",
    "\n",
    ">\n",
    "> __Aside: Why Apache Parquet?__\n",
    ">\n",
    "> Apache Parquet is one of the most popular, efficient, and performant formats for large-scale structured data. \n",
    ">\n",
    "> Why? Because Parquet is a compressed, self-describing, binary *columnar* data format, which means that each column is stored apart from the others. So when we need to query just a few columns in a wide table, we can physically access just the ones we need on disk. The fastest data to process is the data you never load in the first place!\n",
    ">\n",
    "> Moreover, if we know what sorts of queries we will need to perform, we can *partition* by those values on disk as well. In our case, since we're partitioning by CheckoutYear, if we subsequently need records from 2016, we can access those and only those directly from the disk. (This kind of on-disk partitioning is sometimes called \"Hive style\" partitioning, after an Apache Hive pattern.)\n",
    ">\n",
    "> And even if we don't do that sort of on-disk partitioning, we can benefit from metadata stored along with our data.\n",
    ">\n",
    "> Review more details and Parquet format benefits in this Coiled blog post: https://coiled.io/blog/parquet-column-pruning-predicate-pushdown/\n",
    ">\n",
    "\n",
    "Let's `dropna()` and write out our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans3.dropna().to_parquet('cleaned-loans', write_index=False, partition_on='CheckoutYear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>__WARNING and topic for discussion__</span>\n",
    "> \n",
    "> Where did we just write the data? What would happen if we tried to read it back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.environ['WRITE_BUCKET']\n",
    "\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans3.dropna().to_parquet('s3://' + bucket + '/cleaned-loans', write_index=False, partition_on='CheckoutYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "fs.ls(bucket + '/cleaned-loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Progress Dashboard__\n",
    "\n",
    "That seems to take a little while (well, a few seconds, at least). Let's open another dashboard view that will let us track progress.\n",
    "\n",
    "From the Dask dashboard palette, click `Progress` and drag that to snap at the bottom of the JupyterLab window.\n",
    "\n",
    "<img src='images/progress.png' width=900>\n",
    "\n",
    "We'll run the same logic, but -- since we now have a clear spec and a good understanding of the data, we can even compress this workflow into the \"1-liner ETL\"\n",
    "\n",
    "While it's running, you should see several colored progress bars. The colors correspond to specific functions being run (when those are functions you've defined, they'll show your function names; in this case, they're function names from the Dask dataframe library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.read_csv('s3://coiled-training/data/checkouts-small.csv', storage_options={\"anon\": True}) \\\n",
    "    .drop(columns=['Publisher', 'Unnamed: 0']) \\\n",
    "    .query('CheckoutYear >= 2010 & CheckoutYear < 2020') \\\n",
    "    .dropna() \\\n",
    "    .to_parquet('s3://' + bucket + '/cleaned-loans', write_index=False, partition_on='CheckoutYear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a later module, we'll remove some of the magic from the Dask dataframe by giving a little example of how you could build your own ... but first, let's take advantage of our new, structured dataset to query checkouts.\n",
    "\n",
    "We want to track trends in physical vs. digital loans (\"UsageClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ddf.read_parquet('s3://' + bucket + '/cleaned-loans') \\\n",
    "    .groupby(['CheckoutYear', 'UsageClass']).agg({'Checkouts': 'sum'}).compute()\n",
    "    \n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since this is just a Pandas dataframe, we can plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.unstack(level=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Optimizations\n",
    "\n",
    "Parquet allows a number of optimizations at query time, and with the current version of Dask we need to provide some hints in order to take advantage of those capabilities.\n",
    "\n",
    "To illustrate the issue, let's get average checkouts each month for 2017 and 2018. We'll also track the time for these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "report2 = ddf.read_parquet('s3://' + bucket + '/cleaned-loans') \\\n",
    "            .query('CheckoutYear == 2017 | CheckoutYear == 2018') \\\n",
    "            .groupby(['CheckoutYear', 'CheckoutMonth']).agg({'Checkouts': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this query, we're filtering on a *partition column* -- that is, we broke up our data by Checkout specifically to speed up this sort of report. Providing this info to `read_parquet` allows \\\"predicate pushdown\\\" or a \\\"pushed filter\\\" where we only read a minimal set of data from disk.\n",
    "\n",
    "Since this is the complete filtering operation for us, we can skip the `query` call.\n",
    "\n",
    "But how exactly do we specify the filters? The filters kwarg value is\n",
    "* a list of filter groups, where we get UNION (equivalent to OR for all of them) of the filter group results\n",
    "    * in our example, one filtergroup is `CheckoutYear = 2017` and another is `CheckoutYear = 2018` -- we want matches to all (a OR b) of them\n",
    "* where each filter group is a list of filter tuples, and we get results from a filter group when all filters in the tuple are true (AND)\n",
    "    * in our example, there is no AND condition, so our filter groups will just contain one condition each, expressed as filter tuple\n",
    "* a filter tuple is a tuple containing the filter (predicate), broken up into three parts\n",
    "    * the expression to test (in our case, CheckoutYear)\n",
    "    * the condition operator (for us, =)\n",
    "    * the predicate, or value to test against (here, 2017 for the tuple in one filter group, and 2018 for the tuple in the other)\n",
    "\n",
    "That is a mouthful!\n",
    "\n",
    "But it's not too bad in code. Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "report2b = ddf.read_parquet('s3://' + bucket + '/cleaned-loans', \\\n",
    "                filters=[[('CheckoutYear', '=', 2017)],[('CheckoutYear', '=', 2018)]]) \\\n",
    "           .groupby(['CheckoutYear', 'CheckoutMonth']).agg({'Checkouts': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely get a speedup! \n",
    "\n",
    "> NOTE: You may have noticed that we've used the word 'partition' in two different ways so far\n",
    "> 1. to refer to the constituent Pandas dataframes or chunks that make up our bigger Dask dataframe\n",
    "> 2. for dividing up our data by the values in a particular column (for us, CheckoutYear) when we wrote and queried Parquet data\n",
    "> \n",
    "> These two ideas are related but not the same, and they usually refer to different ways of dividing the data.\n",
    "> It's a common source of confusion!\n",
    "\n",
    "In this example, we read from a dataset on disk which we partitioned specifically to support this kind of query.\n",
    "\n",
    "Although that will give you the best performance, it obviously has drawbacks... it's\n",
    "* inflexible (partitioning that supports one set of queries may be more costly for others)\n",
    "* impractical for high-cardinality columns (partitioning by, say, Customer ID would be a very bad idea in most scenarios)\n",
    "\n",
    "Parquet predicate pushdown *can work* when you haven't partitioned on the predicated columns -- but it will return a superset of the requested records (because parquet files store records in row groups and, while Dask can skip row groups where metadata indicates zero interesting records, if there are some interesting records in there, the whole row group will be read) -- so you'll have to filter your data again within your Dask query. There are a couple of other fine points, so definitely check out the docs at https://docs.dask.org/en/latest/generated/dask.dataframe.read_parquet.html#dask-dataframe-read-parquet\n",
    "\n",
    "__We can do even better than with another optimization__\n",
    "\n",
    "This report only really pays attention to 3 columns: CheckoutYear, CheckoutMonth, and Checkouts\n",
    "\n",
    "Since Parquet stores our data in a columnar format, we can provide another hint to only look at these 3 columns.\n",
    "\n",
    "This kind of optimization is called *column pruning*\n",
    "\n",
    "Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "report2c = ddf.read_parquet('s3://' + bucket + '/cleaned-loans', \\\n",
    "                filters=[[('CheckoutYear', '=', 2017)],[('CheckoutYear', '=', 2018)]],\n",
    "                columns=['CheckoutYear', 'CheckoutMonth', 'Checkouts']) \\\n",
    "            .groupby(['CheckoutYear', 'CheckoutMonth']).agg({'Checkouts': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us a further speedup!\n",
    "\n",
    "It's especially nice when we consider that\n",
    "1. these filter and column values are just Python, so you can automate manipulation of them if you need to (e.g., for a reporting tool)\n",
    "2. a future version of Dask will be able to apply this for you automatically, by analyzing the original, simpler query\n",
    "\n",
    "In your own work, the amount of speedup will vary depending on how much data your filters exclude and what sort of computation is going on with the remaining data.\n",
    "\n",
    "For more detail and all of the dataframe docs, bookmark https://docs.dask.org/en/latest/dataframe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task Stream Dashboard__\n",
    "\n",
    "Before we try Dask dataframe out with some lab projects, let's look at one more dashboard. From the palette, choose Task Stream, and snap it somewhere convenient.\n",
    "\n",
    "`%%time` is useful, but doesn't show us a lot of detail about what Dask is doing. The Progress bars are great too, but they don't show the time relationships between different tasks.\n",
    "\n",
    "With the Task Stream open, re-run the previous reports. Zoom in to where you can see individual tasks across your cluster cores -- color coded to match the other views like Progress -- as well as time spent transferring data (the red \"overlay\" boxes\")\n",
    "\n",
    "<img src='images/taskstream.png' width=900>\n",
    "\n",
    "This sort of X-ray vision into what's happening in the cluster makes tuning and troubleshooting a lot easier than doing so with log messages and summary stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Analyze library records\n",
    "\n",
    "*Note: for all of these labs, we'll go back to the original CSV data, not the Parquet data. If you have extra time, you're welcome to investigate whether we can generate them faster via Parquet.*\n",
    "\n",
    "#### Activity 1: Digital media schemes for the city library\n",
    "\n",
    "We need to perform an analysis over time, similar to the \"Digital vs Physical by Year\" report, but we want to compare the various licensing managers for digital media.\n",
    "\n",
    "Essentially, we want to count by year and `CheckoutType` records where\n",
    "* UsageClass is Digital\n",
    "* Year is prior to 2020\n",
    "\n",
    "We want to keep as many records as we can which meet those criteria, and write a reasonably efficient query from the original CSV data.\n",
    "\n",
    "#### Activity 2: Publishers\n",
    "\n",
    "What are the top 50 publishers in the Seattle library system by...\n",
    "* checkout activity (easier)\n",
    "* library material holdings (harder)\n",
    "\n",
    "Hints:\n",
    "* Try to use Dask's `nlargest` or `nsmallest` for ordered results with a limit (like 50).\n",
    "    * That approach is vastly more efficient than trying to sort a big dataset.\n",
    "* For top publishers by library holdings...\n",
    "    * the same item may appear in many months of data\n",
    "    * Pandas/Dask doesn't have the same \"COUNT DISTINCT\" operator as SQL so you may have to get a bit creative\n",
    "    * if you don't narrow (hint!) down the data, it will be hard to run this query with the allocated cluster resources\n",
    "* If your logic works but the computation isn't running successfully, feel free to start over with more memory in your cluster (try 2GB workers instead of 1 GB)\n",
    "\n",
    "#### Activity 3: Popular subjects\n",
    "\n",
    "*Bonus Project*\n",
    "\n",
    "Notice that the Subjects field contains a string list of subjects.\n",
    "\n",
    "If we want to analyze checkouts by subject, we might start by trying to parse this field into a Python list. Like Pandas, Dask allows us to split strings as well as explode collections into multiple rows.\n",
    "\n",
    "Try to find the top 10 subjects by checkout activity. Hint: Try to eliminate as much data as you can from the dataset as early as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Magic\n",
    "\n",
    "There is a lot of power to the Dask dataframe API. But it's not magic. \n",
    "\n",
    "To illustrate a little bit of how a parallel dataframe can work, as well as give insight into how Dask's low-level constructs can be assembled to create high-level ones, we'll build a toy Dask dataframe in a future module (once we've covered the relevant APIs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional key features of Dask and Dask dataframe\n",
    "\n",
    "### Caching\n",
    "\n",
    "One benefit of using a cluster is having more processing power (cores). But equally valuable is having an expanded pool of memory: for example, most of us don't have 250GB of RAM in our laptop, while even a small cluster is likely to have that much memory available.\n",
    "\n",
    "To materialize a Dask dataframe (or any Delayed object) in the distributed RAM of the cluster, we use the `.persist()` API\n",
    "\n",
    "`.persist` is not lazy: it immediately starts working ... but it returns a Delayed right away because we work is not done yet. So we still get a token or handle. And, actually, a token is what we want: the whole point is that we want the big data in the cluster, not in our local Python runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart() # clear out some room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_cached = ddf.read_csv('s3://coiled-training/data/checkouts-small.csv', storage_options={\"anon\": True}).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run some queries or transformations over the data in memory... or can we?\n",
    "How do we know if the data is loaded up yet?\n",
    "\n",
    "There are several ways!\n",
    "\n",
    "First, we can look at the __Graph Dashboard__: from the dashboard palette, click \"Graph\"\n",
    "\n",
    "Each Task (delayed Python function) gets a little square, and the key explains the color coding: red boxes are tasks whose result is stored in memory. \n",
    "\n",
    "For a big job (and a huge graph), we can watch the boxes turn red in real time ... a sort of RAM-storage progress bar.\n",
    "\n",
    "We can also access the information programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "dask.distributed.futures_of(loans_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Future is another kind of handle (similar to Promises in some languages) representing a task that was started but may not have finished (or may have failed altogether). In this example, we can see that each of the Futures is `finished`. \n",
    "\n",
    "We can also wait for all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.distributed.wait(loans_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our queries should be faster with the data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loans_cached.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the non-cached timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf.read_csv('s3://coiled-training/data/checkouts-small.csv', storage_options={\"anon\": True}).mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In practice, your speedups will depend on how expensive the I/O is relative to the computation. \n",
    "\n",
    "The slower, larger, and more distant the source data, the more of an improvement you'll see.\n",
    "\n",
    "On the other hand, the more expensive and complex your computation is, the less improvement you'll see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions with `.apply`\n",
    "\n",
    "Often, you'll want to apply your own logic to data in a Dask dataframe. Like Pandas, Dask supports the `.apply` method to run your own code over rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_length(field):\n",
    "    return len(field)\n",
    "\n",
    "loans_cached.Title.apply(my_custom_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It runs, but does suggest we add some schema information to help out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_cached.Title.apply(my_custom_length, meta=('Title', 'int64')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply to rows, allowing us to perform calculations or transformations depending on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_combo_length(fields):\n",
    "    return len(fields[0]) + len(fields[1])\n",
    "\n",
    "loans_cached[['Title', 'Subjects']].dropna().apply(my_combo_length, axis=1, meta=(None, 'int64')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions are also supported for aggregations and rolling (\"window\") computations.\n",
    "\n",
    "Cached objects can get cleaned up automatically when the client process no longer has handles to the objects... but if we want to a quick reset on cluster memory, we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quirks and limitations\n",
    "\n",
    "As you've probably noticed, Dask dataframe implements a lot of the Pandas API. At the same time, there are also some quirks to get used to (e.g., the schema hints we just provided) as well as functionality that is not implemented ... at least not yet.\n",
    "\n",
    "You can refer to the docs to see which APIs are implemented differently (or not at all). But another approach is to try your planned computation (based on Pandas knowledge) on a small subset of your date -- in a non-destructive way -- and see if it runs and the results check out. Different users will likely prefer one or the other of these tactices.\n",
    "\n",
    "### Best practices\n",
    "\n",
    "Some additional best practices for working with Dask dataframe as well as patterns/anti-patterns are documented here\n",
    "* https://docs.dask.org/en/latest/dataframe-best-practices.html\n",
    "* https://docs.dask.org/en/latest/dataframe.html#common-uses-and-anti-uses\n",
    "\n",
    "Coiled has some short, useful blog posts explaining...\n",
    "* Repartitioning dataframes and dealing with small/empty partitions https://coiled.io/blog/repartition-dataframe/\n",
    "* Using the index to select within dataframes, and Dask's 'known divisions' index concept https://coiled.io/blog/filter-dataframes-loc/\n",
    "* Calculating memory size of dataframes and partitions https://coiled.io/blog/dask-memory-usage/\n",
    "    * Especially valuable since you'll want to maintain a relationship between partition sizes and worker memory size\n",
    "\n",
    "Common scenarios are explained in the docs, including...\n",
    "* Shuffles https://docs.dask.org/en/latest/dataframe-groupby.html\n",
    "* Joins https://docs.dask.org/en/latest/dataframe-joins.html\n",
    "* Categorical types https://docs.dask.org/en/latest/dataframe-categoricals.html\n",
    "\n",
    "And if you're curious about how it all works, a design description of the internals is at https://docs.dask.org/en/latest/dataframe-design.html ... from there you can take a look at source if you'd still like see more.\n",
    "\n",
    "### Swappable partition dataframe implementations and RAPIDS cuDF\n",
    "\n",
    "Since Dask dataframe is architected around proxying to Pandas dataframes ...\n",
    "and Python allows us to swap in alternative objects, provided they implement the same protocol or interface (\"duck typing\") ...\n",
    "we can use Dask with other dataframe implementations.\n",
    "\n",
    "Most notably, this support scalable GPU-based dataframes but placing Dask on top of cuDF dataframes in NVIDIA RAPIDS\n",
    "* https://docs.rapids.ai/api/cudf/stable/10min.html#\n",
    "* https://docs.rapids.ai/api/cudf/stable/dask-cudf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
