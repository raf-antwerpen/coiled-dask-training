{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a Real-World Use Case: Batch ML Scoring \n",
    "\n",
    "Did you get an email in your inbox today offering product promotions and deals -- customized just for you?\n",
    "\n",
    "If so, you've already experienced the common real-world use case we're about to explore: batch scoring with a machine learning model.\n",
    "\n",
    "In batch scoring, we have a trained model and a large number of records. We'd like to parallelize the scoring of these records so that we can generate predictions -- like what to include in your promo email -- in a short period of time.\n",
    "\n",
    "Unlike near-real-time scoring (like the models which determine whether to flag a credit card swipe as fraud), batch scoring has an advantage: it doesn't need to happen in milliseconds. It also has a disadvantage: instead of scoring for one person or one credit card, there might be hundreds of millions of records involved.\n",
    "\n",
    "Let's put everything we've learned together and implement this scenario *the right way* with Dask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "Here's what we have so far\n",
    "\n",
    "* a trained PyTorch model\n",
    "    * in our example, a convolutional network trained on Fashion MNIST to recognize clothing items\n",
    "        * https://github.com/zalandoresearch/fashion-mnist\n",
    "* a bunch of records we want to score\n",
    "    * we'll use 10,000 but the goal is to design this to scale this arbitrarily large\n",
    "* Dask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We know that Dask+Python are flexible enough that we could do this in a lot of ways and it would probably run.__\n",
    "\n",
    "But we also know that if we don't plan ahead, we might see suboptimal performance due to things like\n",
    "* moving too much data\n",
    "* moving data to/from the wrong places (e.g., distant storage or a bottleneck like the process hosting the client)\n",
    "* tasks that are too short or too long in duration\n",
    "* tasks that waste cycles repeating common logic\n",
    "* instability due to running too close to resource limits\n",
    "* or other issues\n",
    "\n",
    "__So let's make a plan__\n",
    "\n",
    "### Input data\n",
    "\n",
    "The input data are the records we want to score. \n",
    "\n",
    "#### Location\n",
    "\n",
    "We'd like this to be in a place that all of our workers can read from in parallel and without excessive network transport.\n",
    "\n",
    "Since our compute will happen in AWS, we'd read source data in from an S3 bucket.\n",
    "\n",
    "#### Format\n",
    "\n",
    "We are scoring image data, which is effectively array data.\n",
    "\n",
    "We should use an efficient format like HDF or Zarr.\n",
    "\n",
    "#### Lazy access\n",
    "\n",
    "Dask's Array will give us the lazy-read characteristics we want. Handling the shape/chunking should be no problem since we know the details of the data ... __but__ we may want to revisit chunking along the batch (record number) axis. We'll come back to this as we talk about processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "arr = da.from_zarr('s3://coiled-training/data/images_to_score', storage_options={\"anon\": True})\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.rechunk(100, 28, 28)\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the ML Processing ... \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/jerry.gif' width=480>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Not quite yet ... \n",
    "\n",
    "### More data: the model\n",
    "\n",
    "In this use case, our model is actually a hefty piece of data. Our small Fashion MNIST model is 2.5MB, which we don't want to transport every time we score records, and a more \"serious\" model might easily be in the GB range (to say nothing of state-of-the-art language models).\n",
    "\n",
    "There are a few options. We can load the model from shared storage like S3 or a model database for maximum flexibility, or -- if we are creating a container config for this specific application -- we could bake it into the container image.\n",
    "\n",
    "Each worker will need to load the model at least once. But Dask is smart enough to figure this out, and move the model around the cluster as needed ... we just need a token (`Future`) that points to the model, and we can get this by calling `client.submit` on a helper that loads the model once.\n",
    "\n",
    "> For models that are not serializable in the standard way, or where Dask cannot automate replication effectively, we can maually set up the model for each worker via `client.run`. There is an example notebook illustrating that approach, but this \"automatic\" one is preferable, and easier to think about\n",
    "\n",
    "First we'll test locally (as a sanity check, since remote execution is tougher to troubleshoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "\n",
    "file = 'https://coiled-training.s3.amazonaws.com/data/fmnist.pyt'\n",
    "\n",
    "with urllib.request.urlopen(file) as f:\n",
    "    model = torch.load(BytesIO(f.read()))\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    file = 'https://coiled-training.s3.amazonaws.com/data/fmnist.pyt'\n",
    "    with urllib.request.urlopen(file) as f:\n",
    "        model = torch.load(BytesIO(f.read()))\n",
    "    return model\n",
    "\n",
    "model_future = client.submit(load_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "We can keep it simple and use a Dask Array `map_blocks` approach. We may need to tune these blocks later, but for now let's recall that our data looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(block):\n",
    "    return np.random.normal(size=(block.shape[0], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score(arr.blocks[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.map_blocks(test_score, chunks=(100,10), dtype=np.float32, drop_axis=2).compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(block, model):\n",
    "    torch_tensor = torch.tensor(block)\n",
    "    reshaped = torch_tensor.view(-1, 1, 28, 28)\n",
    "    rescaled = reshaped / 255.0\n",
    "    raw_scores = model(rescaled)\n",
    "    scores = torch.softmax(raw_scores, 1).detach().numpy()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_to_outputs = arr.map_blocks(score, model_future, chunks=(100,10), dtype=np.float32, drop_axis=2)\n",
    "handle_to_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_to_outputs[:3, :].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_to_outputs[:3, :].argmax(axis=1).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing output\n",
    "\n",
    "We want to write the output in parallel from the scoring tasks, to shared storage.\n",
    "\n",
    "Let's store the class probabilities in the same order as the source records, in zarr format, to shared storage. \n",
    "\n",
    "## The Execution\n",
    "\n",
    "Now that we've planned everything out, the write command will kick it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.environ['WRITE_BUCKET']\n",
    "\n",
    "handle_to_outputs.to_zarr('s3://' + bucket + '/scores', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "The last step is to see what our resource consumption looks like, and tune our job. \n",
    "\n",
    "Mainly, we want to optimize the number of records scored per batch, so that we are making good use of the Dask Scheduler.\n",
    "\n",
    "Since the batches are currently aligned with array blocks -- a good practice -- changing the batch size may mean changing array chunk size.\n",
    "\n",
    "> In my sample run, I have a lot of very short tasks. The actual duration will depend on hardware and other factors. But we could go to 10x or more with the amount of data and aim for a longer task time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_task_output = arr.rechunk(2000, 28, 28).map_blocks(score, model_future, chunks=(1000,10), dtype=np.float32, drop_axis=2)\n",
    "bigger_task_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_task_output.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the memory spike a little bit ... In a real-life scenario, we could run out of memory or spill to disk with a much larger data chunk ... but an even more likely scenario is running out of GPU memory if we are using GPU. Since we want to \"load up\" our GPUs with as many records as we can fit, we'll often be on the edge of GPU mem limits, and unfortunately those are harder to increase :)\n",
    "\n",
    "We can easily rechunk to use less memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_task_output = arr.rechunk(500, 28, 28).map_blocks(score, model_future, chunks=(500,10), dtype=np.float32, drop_axis=2)\n",
    "bigger_task_output.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last experiment, I'm getting somewhat longer tasks, and not running out of memory.\n",
    "\n",
    "For a real-world deployment, we would keep tuning, and probably use larger workers and even bigger chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_task_output.to_zarr('s3://' + bucket + '/scores', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we've illustrated the key end-to-end workflow...\n",
    "\n",
    "Next steps might be to try with even larger datasets and make sure we retain both good parallel utilization, good task size, and no failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Lab Activity\n",
    "\n",
    "In our data folder, there's another version of this same model, in an open industry-standard format called ONNX. \n",
    "\n",
    "You can learn more about ONNX at https://onnx.ai/\n",
    "\n",
    "There are a variety of reasons to use an open, standard format for deploying your models -- and ONNX can handle a huge variety of models: \n",
    "* https://onnx.ai/supported-tools.html\n",
    "* https://github.com/onnx/onnxmltools\n",
    "\n",
    "ONNX features a number of runtime (deployment) options. One is an open source implementation for CPU (and another for GPU) from Microsoft: https://microsoft.github.io/onnxruntime/\n",
    "\n",
    "Try replicating what we did here in this lab, but using the ONNX model artefact and the `onnxruntime` library for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
