{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Clusters\n",
    "\n",
    "*Cascadia City's CTO hasn't decided if the cloud makes sense, or on-prem Kubernetes, or something else entirely. But that hasn't added any slack to our schedule: we need to demo a Dask cluster and explain it in simple terms. The mayor has offered us a server to install on...*\n",
    "\n",
    "We'll start with the most simple and least automated, which will allow us to easily see the machinery \"under the hood,\" although this manual approach is not likely the best solution for real deployments.\n",
    "\n",
    "After we're familiar with the core components, we'll review a series of tools that let us deploy and manage clusters in a more scalable and devops-friendly way.\n",
    "\n",
    "<img src='images/dask.svg' width=800>\n",
    "\n",
    "## Dask's Cast of Characters\n",
    "\n",
    "### The Scheduler\n",
    "\n",
    "Let's start a scheduler.\n",
    "\n",
    "1. To make it simple to see what's happening, kill any running Jupyter kernels.\n",
    "2. Open a new terminal in Jupyter\n",
    "3. Type `dask-scheduler`\n",
    "\n",
    "After a few seconds, you should see output that looks something like this\n",
    "\n",
    "```\n",
    "distributed.scheduler - INFO - -----------------------------------------------\n",
    "distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
    "distributed.scheduler - INFO - -----------------------------------------------\n",
    "distributed.scheduler - INFO - Clear task state\n",
    "distributed.scheduler - INFO -   Scheduler at:    tcp://192.168.1.5:8786\n",
    "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
    "```\n",
    "\n",
    "This tells us how workers will talk to the scheduler (the `tcp://` uri) and how we can view the dashboards (the port underneath). Notice that the scheduler process is serving the main dashboard. That's why, when the scheduler is \"bogged down\" with work, the dashboards are less responsive. \n",
    "\n",
    "__What does this scheduler do?__\n",
    "\n",
    "The scheduler is a Python process which serves as the brains of the Dask cluster. The scheduler ...\n",
    "* is the component that users communicate with to schedule work\n",
    "* decides which work to send where on the cluster\n",
    "* optimizes or rearranges task graphs for better throughput\n",
    "\n",
    "Just to be extra clear on what's happened, open another terminal in Jupyter and run\n",
    "\n",
    "`ps -efww | grep python`\n",
    "\n",
    "You should be able to identify your dask-scheduler process. Note its PPID (parent process ID). Now run\n",
    "\n",
    "`ps -efww | grep <ppid>` where you replace `<ppid>` with the PPID you found above. That should be the shell you used to start the scheduler.\n",
    "\n",
    "> __Tip:__ Command-line options for the scheduler are at https://docs.dask.org/en/latest/setup/cli.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Worker\n",
    "\n",
    "We could try and talk to the scheduler and run some work ... but so far there's no one to actually do the work.\n",
    "\n",
    "Remember: the scheduler plans the work, mediates communication, and can even help with managing metadata ... but it doesn't actually run your real Python workload tasks.\n",
    "\n",
    "For those, we need at least one worker.\n",
    "\n",
    "1. Open a new terminal in Jupyter\n",
    "2. Type `dask-worker --no-nanny <tcp://...>` where `<tcp://...>` is replaced with the `tcp://` uri from the scheduler output\n",
    "\n",
    "You should see output something like this:\n",
    "```\n",
    "distributed.worker - INFO -       Start worker at:    tcp://192.168.1.5:60278\n",
    "distributed.worker - INFO -          Listening to:    tcp://192.168.1.5:60278\n",
    "distributed.worker - INFO -          dashboard at:          192.168.1.5:60279\n",
    "distributed.worker - INFO - Waiting to connect to:     tcp://192.168.1.5:8786\n",
    "distributed.worker - INFO - -------------------------------------------------\n",
    "distributed.worker - INFO -               Threads:                          8\n",
    "distributed.worker - INFO -                Memory:                   17.18 GB\n",
    "distributed.worker - INFO -       Local Directory: /foo/bar/baz/dask-worker-space/worker-hratgouz\n",
    "distributed.worker - INFO - -------------------------------------------------\n",
    "distributed.worker - INFO -         Registered to:     tcp://192.168.1.5:8786\n",
    "distributed.worker - INFO - -------------------------------------------------\n",
    "distributed.core - INFO - Starting established connection\n",
    "\n",
    "```\n",
    "\n",
    "Choose the Jupyer tab you used earlier to run `ps` and try it again (filtering for python).\n",
    "\n",
    "You should see, aside from your Jupyter process, the scheduler and the worker and nothing else.\n",
    "\n",
    "> Command line options for the worker process are also at https://docs.dask.org/en/latest/setup/cli.html\n",
    "\n",
    "__What does this worker do?__\n",
    "\n",
    "The worker will execute tasks -- Python functions -- for the end user, once we have a way to send work to it via the scheduler.\n",
    "\n",
    "The worker also has its own dashboard, which you can access at the URL indicated.\n",
    "* Add `/status` to the URL\n",
    "* If you do not have direct routing to the indicated host and port, you won't be able to load the dashboard directly.\n",
    "    * You may be able to get to it by identifying your Jupyter URL and using the /proxy/<port> functionality\n",
    "\n",
    "Flip back to the terminal tab hosting the scheduler. You should see additional output like this\n",
    "    \n",
    "```\n",
    "distributed.scheduler - INFO - Register worker <Worker 'tcp://192.168.1.5:60278', name: tcp://192.168.1.5:60278, memory: 0, processing: 0>\n",
    "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.5:60278\n",
    "```\n",
    "  \n",
    "Indicating the scheduler has registered the worker and could send it tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Client\n",
    "\n",
    "Time to get to work!\n",
    "\n",
    "Let's create a `Client` object and use it to run some code.\n",
    "\n",
    "1. Open another terminal tab\n",
    "2. Run `ipython`\n",
    "3. Enter the following code, substituting your scheduler's uri for `<scheduler>`\n",
    "\n",
    "```\n",
    "from distributed import Client\n",
    "\n",
    "c = Client('<scheduler>')\n",
    "```\n",
    "\n",
    "You should see output like\n",
    "`<Client: 'tcp://192.168.1.5:8786' processes=1 threads=8, memory=17.18 GB>`\n",
    "\n",
    "Meanwhile, the scheduler's output should add\n",
    "\n",
    "`distributed.scheduler - INFO - Receive client connection: Client-c56e5898-f5ee-11ea-a83b-3c15c2cadbf2`\n",
    "\n",
    "and `ps` should show your `ipython` process __but no additional process for the Client, since the Client is just a Python object in-process with the code that uses it (in this case, IPython shell)__\n",
    "\n",
    "Now let's try running some work:\n",
    "\n",
    "```python\n",
    "def add_numbers(to): \n",
    "    return sum(range(to)) \n",
    "                                                                                                                                                   \n",
    "f = c.submit(add_numbers, 1000)\n",
    "\n",
    "f.result()\n",
    "```\n",
    "\n",
    "You should see the result (499500). If you're not totally convinced that you've used the scheduler to run a task on the worker, you can check:\n",
    "\n",
    "Your main dashboard should show the `add_numbers` task in the Task Stream, Memory by Key, and Task Graph\n",
    "\n",
    "How can we see where this task ended up? Click the `Info` tab, then the URL link for your worker. Drilling down, you should get to a screen like this, indicating your worker's details and the `add_numbers` task details.\n",
    "\n",
    "<img src='images/worker.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nanny\n",
    "\n",
    "__Motivating the Nanny__\n",
    "\n",
    "In the terminal tab you've been using for `ps`, run that `ps` command again and identify the PID of the worker.\n",
    "\n",
    "Kill it: `kill <PID>`\n",
    "\n",
    "The scheduler now emits\n",
    "\n",
    "```\n",
    "distributed.scheduler - INFO - Remove worker <Worker 'tcp://192.168.1.5:60278', name: tcp://192.168.1.5:60278, memory: 1, processing: 0>\n",
    "distributed.core - INFO - Removing comms to tcp://192.168.1.5:60278\n",
    "distributed.scheduler - INFO - Lost all workers\n",
    "```\n",
    "\n",
    "Let's wait a second and see if things improve.\n",
    "\n",
    "Spoiler alert: not really. \n",
    "\n",
    "Try running the IPython code to submit a task again.\n",
    "\n",
    "Nothing... go ahead and hit CTRL-C\n",
    "\n",
    "__What do we do about our worker?__\n",
    "\n",
    "We *could* manually start a new one. \n",
    "\n",
    "If our worker dies for an unknown/unwanted reason, like in the example above, we probably want a new worker. We could write a script that does this for us. We might want that script to keep track of other issues in the worker, like if it's running out of resources.\n",
    "\n",
    "These common uses cases motivate the __Nanny__, a process that serves *in loco parentis* of our worker, to keep an eye on it.\n",
    "\n",
    "In the terminal where we started our worker earlier, go \"up\" a line in the history and remove `--no-nanny` and run the command like this:\n",
    "\n",
    "`dask-worker <tcp://...>` where <tcp://...> is the scheduler address.\n",
    "\n",
    "You should see output like earlier, except, this time, the first line should read\n",
    "\n",
    "`distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.5:62444'`\n",
    "\n",
    "__Check that \"it works\"__\n",
    "\n",
    "1. In the terminal tab where your scheduler lives, check that a worker is registered\n",
    "2. In your IPython tab, run the code again -- you should get the result\n",
    "3. In your `ps` tab, run the PS again...\n",
    "    * You should see a `dask-worker` -- that's actually the nanny (\"parent\") process\n",
    "    * You should see two entries that look like `python -c from multiprocessing...`\n",
    "        * Ignore the one that says `semaphore_tracker` for now; that's a resource-tracking helper\n",
    "        * The one that says `from multiprocessing.spawn...` is the worker\n",
    "4. Kill the PID corresponding to the worker\n",
    "\n",
    "Look at the worker's terminal tab. You should see two messages from the nanny:\n",
    "```\n",
    "distributed.nanny - INFO - Worker process 10511 was killed by signal 15\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "```\n",
    "\n",
    "before the worker's greeting\n",
    "\n",
    "`distributed.worker - INFO -       Start worker at:    tcp://192.168.1.5:62767`\n",
    "\n",
    "Just to wrap up, run your IPython code block again. All should be well.\n",
    "\n",
    "> __What if the nanny dies?__\n",
    ">\n",
    "> Go ahead and try it\n",
    ">\n",
    "> You'll notice that, as the parent process, if the nanny dies then it's game-over for that worker\n",
    ">\n",
    "> Doesn't that mean the nanny is pointless, and just \"kicks the can down the road\" in terms of failure?\n",
    ">\n",
    "> Not really: if the goal were to prevent chaos-monkey style random process termination, then yes.\n",
    ">\n",
    "> But the true purpose of the nanny is to recover when the worker dies \"from inside\" -- i.e., some user code causes a fatal fault, the worker runs out of memory, etc.\n",
    ">\n",
    "> *In other words, the nanny is not there to keep your cluster running from an outside, devops perspective, but rather from an inside, Dask+user perspective*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guest star: the cluster resource manager\n",
    "\n",
    "By cluster resource resource manager, we mean an external, pre-existing piece of architecture that manages the pieces of your Dask cluster, and makes it easy to run multiple Dask clusters side-by-side.\n",
    "\n",
    "Examples include\n",
    "* Kubernetes\n",
    "* YARN\n",
    "* Coiled Cloud or other providers-as-a-service\n",
    "\n",
    "These players are __not__ part of Dask itself.\n",
    "\n",
    "They are \"guest stars\" in that they will usually be part of your Dask show, because they add a lot of capability and usability, save a lot of time and money, and will end up in your Dask operations plan.\n",
    "\n",
    "The purpose of the cluster resource manager is to \n",
    "* save you from ever having to start and configure the scheduler, workers, etc. on your own\n",
    "* provide a uniform, single location to specify configuration\n",
    "    * including, usually, a container spec that simplifies provisioning dependencies to workers\n",
    "* allow for a simple cluster scaling API, so that end users can programmatically change their cluster's size without direct access to any underlying services or machines\n",
    "\n",
    "We usually encounter the cluster resource manager when creating a cluster. We use a helper library that provides an implementation of `Cluster`\n",
    "\n",
    "Examples include\n",
    "* `SSHCluster` (built in)\n",
    "* dask-jobqueue or dask-drmaa and their associated cluster classes like `PBSCluster`\n",
    "* dask-kubernetes (`KubeCluster`, `HelmCluster`)\n",
    "* dask-yarn (`YarnCluster`)\n",
    "* dask-cloudprovider (`FargateCluster`, `AzureMLCluster`)\n",
    "* coiled (`coiled.Cluster`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Clusters\n",
    "\n",
    "In this section, we'll discuss creating and managing clusters.\n",
    "\n",
    "As an example, we'll create a cluster using the managed Coiled Cloud service.\n",
    "\n",
    "The goal here is not to advertise Coiled but rather to \n",
    "* Show the Cluster API\n",
    "* Discuss the aspects of cluster management that Coiled provides, and which large projects, groups, or institutions need to attend to *regardless* of how they choose to deploy Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What sort of cluster to we get? How can we control it?\n",
    "\n",
    "There are two main categories of configuration, which are often at least somewhat independent\n",
    "\n",
    "__Cluster configuration__\n",
    "* number of workers\n",
    "* cores per worker\n",
    "* memory per worker\n",
    "* use of nanny process\n",
    "* certificate info for TLS\n",
    "* potentially separate config for the scheduler vs. the workers\n",
    "* and others, typically less important\n",
    "    \n",
    "__Software environment configuration__\n",
    "* software packages\n",
    "* package version requirements\n",
    "* resource files\n",
    "* anything else you might want on your cluster\n",
    "    \n",
    "In Coiled Cloud, these are configurable through\n",
    "* Web GUI\n",
    "    * https://cloud.coiled.io/{username}/software    \n",
    "* CLI\n",
    "    * https://docs.coiled.io/user_guide/cluster.html\n",
    "    * https://docs.coiled.io/user_guide/software_environment.html\n",
    "    \n",
    "__Other common environments__\n",
    "\n",
    "For the `LocalCluster`s we've created throughout the class,\n",
    "* we passed cluster info to the `LocalCluster` constructor\n",
    "* software environment was inherited from the Python environment where we were already working\n",
    "* naturally, this is simple to operate, but doesn't scale, since it's __local__ to our machine only\n",
    "    \n",
    "A common distributed configuration is the Kubernetes-hosted `KubeCluster`\n",
    "* cluster spec information is provided through any of...\n",
    "    * the `KubeCluster` constructor https://kubernetes.dask.org/en/latest/api.html#dask_kubernetes.KubeCluster\n",
    "    * `from_dict` or `from_yaml` class methods on `KubeCluster` https://kubernetes.dask.org/en/latest/kubecluster.html\n",
    "    * the config/spec can also be placed in the filesystem instead of supplied programmatically\n",
    "* software environment is implicit in the *container image* used for the workers and/or scheduler\n",
    "    * container image is part of a *pod template*\n",
    "    * the pod templates are part of the same info through which the cluster is configured\n",
    "        \n",
    "Similar, though slightly different, patterns are available for, e.g., `YARNCluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(cluster.scheduler_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run work, we can create a client from the cluster instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(lambda x:x*x, 3).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cluster object API depends on which implementation you are using, but most implementations allow for scaling.\n",
    "\n",
    "We can request to scale to a particular number of workers (all based on our current worker and software config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(4) # this can take some time if we're provisioning machines/containers in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling down is usually much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Scaling\n",
    "\n",
    "Most clusters also support the `adapt` method, assuming they are either local, or on a distributed system which is able to allocate and deallocate workers dynamically.\n",
    "\n",
    "The fundamental principles of this auto-scaling mechanism are at https://docs.dask.org/en/latest/setup/adaptive.html\n",
    "\n",
    "The key pieces are summarized here:\n",
    "Dask...\n",
    "* has timing data on all previous tasks via its builtin profilers\n",
    "* uses that info to estimate future task durations\n",
    "* combines that info, along with the resources available on the workers, to estimate future time per task\n",
    "* and finally creates a scaling target with the goal of matching total with a `target_duration` parameter (default: 5 seconds)\n",
    "\n",
    "The `adapt` API offers several sets of optional limits that we can supply\n",
    "* min/max number of workers\n",
    "* min/max total cores\n",
    "* min/max total memory\n",
    "\n",
    "`adapt` can also accept an `Adaptive` object with a few more parameters https://distributed.dask.org/en/latest/api.html?highlight=scale#adaptive\n",
    "\n",
    "> __Bonus:__ as we will see in a subsequent module on resilience, the `adapt` call will be useful to ensure that the number of workers stays within desired bounds even if workers fail.\n",
    "\n",
    "#### Optional Scaling Demo Mini-Lab\n",
    "\n",
    "Scaling -- both explicit and adaptive -- is easier to see (if a bit less realistic) when using `LocalCluster`. Since launching a worker is just starting a process in the `LocalCluster`, it happens nearly instantly. For a fun exercise, \n",
    "1. Create a `LocalCluster` with code like this\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit='512MiB')\n",
    "client = Client(cluster)\n",
    "client\n",
    "```\n",
    "\n",
    "2. Start the cluster\n",
    "3. Open the `Cluster Map` dashboard widget\n",
    "4. Try some manual scaling and see what happens\n",
    "5. Now turn on adaptive scaling\n",
    "6. Kick off a workload and watch the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Administrative concerns\n",
    "\n",
    "While the APIs make the user-facing features fairly straightforward, these configurations -- and the different options available via different cluster managers (e.g., k8s, YARN, etc.) -- raise certain concerns around administration and manageability.\n",
    "\n",
    "### Seeing and specifying config\n",
    "\n",
    "Although programmatic APIs to configure individual Dask behaviors are great in our coder or analyst role, from an admin point of view it is often useful (or critical) to inspect the current config and to declaratively specify config.\n",
    "\n",
    "Current runtime config is exposed via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "dask.config.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config can be programmatically modified through `dask.config` APIs and and declarative config can be specified through files and/or environment variables, as described at\n",
    "\n",
    "> https://docs.dask.org/en/latest/configuration.html\n",
    "\n",
    "The complete config reference is at: https://docs.dask.org/en/latest/configuration-reference.html\n",
    "\n",
    "### Managing software environments\n",
    "\n",
    "Containerization is a step in the right direction, but even containers and a registry don't solve every problem. For example, sharing configurations, keeping dependency versions synchronized but also up-to-date (especially around internal, rapidly changing code), interacting with CI and build systems are all important concerns, though outside the scope of Dask itself.\n",
    "\n",
    "Hosted services (like Coiled) and cloud-native patterns (like Kubernetes) provide the simplest approach.\n",
    "\n",
    "Other systems, e.g., YARN, are more complex and will likely require additional tooling to keep everything automatically synchronized in an enterprise or large-institution setting (https://yarn.dask.org/en/latest/environments.html)\n",
    "\n",
    "### Quotas, cost administration, and reporting\n",
    "\n",
    "Dask itself does not have a fine-grained quota enforcement system nor cost-tracking mechanisms. \n",
    "\n",
    "Furthermore, even if there are no specific limits or tracking required, an organization may want or need to produce reports on activity, prodictivity, etc. at different levels of granularity.\n",
    "\n",
    "While some of this information can be extracted from Dask's metrics (with some dedicated code), these are not high-level user-visible featured of Dask. These concerns which often motivate cloud-based solutions, third-party products, or custom development at large institutions.\n",
    "\n",
    "### Security\n",
    "\n",
    "Dask supports point-to-point encryption via TLS, and -- to a limited extent -- an organization's PKI and certificate infrastructure can assist in security by making it harder (or easier) for different individuals/units/groups to communicate with particular endpoint. \n",
    "\n",
    "For example, in an unsecured environment, I can connect a `Client` to any scheduler I am able to route to. If the scheduler requires a particular certificate, or a cert issued from a particular CA, then that vulnerability may be limited.\n",
    "\n",
    "On the whole, however, Dask is about computation more than fine-grained security. It allows users to run potentially untrusted code remotely, and to do so in shared environments whose state (and other users) they may not fully know or trust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
