{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping Results Growing on the Server Farm:<br> Data and Resiliency\n",
    "\n",
    "We've looked at passing data from task to task, emphasizing leaving results in the cluster and letting Dask's scheduler place tasks to minimize data movement.\n",
    "\n",
    "<img src='images/pumpkin.jpg' width=400>\n",
    "But let's get really detailed about the three big concerns around data movement:\n",
    "\n",
    "1. Supplying input data to your computations\n",
    "2. Storing the results of computations\n",
    "3. What happens when intermediate data exceeds cluster memory\n",
    "\n",
    "We'll also look at general resilience, performance, and debugging issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = coiled.Cluster(name=\"training-cluster\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing data to begin a computation\n",
    "\n",
    "### Implicit read from client process\n",
    "\n",
    "For (very) small amounts of source data, we can load it \"implicitly\" from our local process (where the `Client` is running) via parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(a_list):\n",
    "    return sum( (a*a for a in a_list) )\n",
    "\n",
    "r1 = client.submit(sum_of_squares, [1,2,3])\n",
    "r1.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or outer-scope references ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_scope_list = [1,2,3]\n",
    "\n",
    "def sum_of_squares():\n",
    "    return sum( (a*a for a in outer_scope_list) )\n",
    "\n",
    "r2 = client.submit(sum_of_squares)\n",
    "r2.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this will technically work for larger chunks of data (up to a point), it will degrade performance, since it makes our local process into a bottleneck for data loading.\n",
    "\n",
    "### Explicit read from a shared source\n",
    "\n",
    "We want to load large amounts of data from some shared location, in parallel. The best-case scenario for loading data is for the tasks to get it directly from a shared filesystem that is both network-local and very fast.\n",
    "\n",
    "If your __Task Stream__ and __Graph__ dashboards aren't open, open those up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\"s3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv\", \n",
    "                 storage_options={\"anon\": True},\n",
    "                 blocksize='32MB')\n",
    "df.partitions[:2].map_partitions(len).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the Task Stream and Graph, we're processing these in parallel from the source.\n",
    "\n",
    "### Use of cached results\n",
    "\n",
    "We've seen that we can cache datasets in the cluster memory via `.persist()` and get back handles to cached data as well as `Future`s we can use to track status.\n",
    "\n",
    "We can then use these handles (to cached data structures) to run subsequent operations. This approach is a great way to supply data to computations *if* the caching makes sense in the first place.\n",
    "\n",
    "When does caching makes sense?\n",
    "\n",
    "* Caching makes sense when we (or others using our cluster) will be making multiple uses of the stored data\n",
    "    * (if we only ever use a dataset once, there's no benefit to caching it)\n",
    "* __and__ we can fit much/most/all of it in memory\n",
    "* __and__ we feel that the RAM is better used for this dataset than another dataset we may want to cache\n",
    "* __and__ the form of the data in RAM is a sensible relative to alternatives\n",
    "    * e.g., if we have a wide table cached in RAM, and we expect to run multiple queries over it, but perhaps using only a few columns or using coarse-grained predicates that lend themselves to on-disk partitioning, then we might do better to make a local scratch copy in Apache Parquet format, or trimmed down, etc.\n",
    "* Conversely, if the alternative is expensive and time-consuming network reads from distant locations, then caching may make sense even when some of the above conditions don't hold.\n",
    "    * There are definitely \"gray areas\" ... for example, suppose you can't fit keep the whole dataset in RAM the whole time, so some parts are spilling to disk. That may still be faster than some remote reads.\n",
    "    \n",
    "The purpose of these guidelines is *not* to discourage caching -- after all, you're paying for a bunch or RAM and you should try to get maximum use from it -- but rather to point out that caching should be a part of your design which you think about. Putting `.persist()` throughout your code without further consideration is not a good pattern.\n",
    "\n",
    "### Explicitly distributing local data\n",
    "\n",
    "Sometimes we have datasets which are not accessible/addressable directly for the workers, and too large to \"implicitly\" send with tasks.\n",
    "\n",
    "> Example: MB or GB scale datasets that we have intentionally created or retrieved to the client, and which we want to modify locally and then supply for future computation.\n",
    "\n",
    "Dask's `scatter` feature will distribute these objects to workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "medium_array = np.random.uniform(0, 1, (100, 100, 100))\n",
    "sys.getsizeof(medium_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = client.scatter(medium_array)\n",
    "f.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the object lands on one worker -- look for the `ndarray` result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.has_what()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we break up our data and `scatter` a list, it will be distributed across workers round-robin in proportion to their cores. As with task submission, specific worker destination(s) can be specified if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = medium_array[:50]\n",
    "part2 = medium_array[50:]\n",
    "\n",
    "f2 = client.scatter([part1, part2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.has_what()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the futures as parameters to any task that needs that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(np.sum, f2[0]).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data (or expensive handles) in the cluster between computations\n",
    "\n",
    "In between operations, we may need to store large results or other expensive items (e.g., connection handles which are small but expensive to open and not serializable, so they cannot persist or traverse the network).\n",
    "\n",
    "### Implicit worker storage of results\n",
    "\n",
    "We've seen that the results of computations which we may still need are implicitly stored in the cluster memory. We can refer to these refer indirectly with futures.\n",
    "\n",
    "But the cluster has memory limits. As memory fills up, Dask will spill this data to disk ... and may eventually have to restart the worker.\n",
    "\n",
    "The core heuristic works like this:\n",
    "\n",
    "1. At 60% of memory load (as estimated by `sizeof`), spill least recently used data to disk\n",
    "2. At 70% of memory load, spill least recently used data to disk regardless of what is reported by sizeof\n",
    "3. At 80% of memory load, stop accepting new work on local thread pool\n",
    "4. At 95% of memory load, terminate and restart the worker\n",
    "\n",
    "Even if the worker never reaches stage 3 or 4, data will be spilled to local disk.\n",
    "\n",
    "__The location for spill__ is the `local_directory` with which the `Worker` is instantiated (`--local-directory` if launched from the command line); if that's not present, it falls back to the `temporary-directory` Dask config option, and after that to the OS current working directory of the Worker's process. \n",
    "\n",
    "__Why mention that detail?__ Because configuring that to point to a fast local scratch space will improve overall performance, especially in operations that are likely to spill due to large intermediate results. Conversely, slow storage (spinning disk as opposed to SSD/NVMe) or network-mounted storage can degrade performance.\n",
    "\n",
    "In general, memory is a critical resource and swapping to disk, while sometimes necessary, can impose hard-to-see costs. This blog post -- https://coiled.io/blog/tackling-unmanaged-memory-with-dask/ -- describes some recent improvements to Dask's memory management as well as dashboard reporting.\n",
    "\n",
    "### Manual storage of temporary/intermediate results\n",
    "\n",
    "Your tasks are free to persist their own intermediate results, large or not-so-large, to a cluster-accessible filesystem or database. Currently, Dask does not include such a scratch facility, but a colocated system like Redis can be added if necessary.\n",
    "\n",
    "In some cases, even a slower local storage medium -- say, a cluster-local shared filesystem like Minio -- can yield significant gains.\n",
    "\n",
    "Consider the earlier example of loading NYC taxi data from S3 and imagine we want to perform lots of analytic queries on this dataset. Suppose further that we can't keep it all in cluster memory. Manually creating a local cache, in a performant format like Apache Parquet, may be vastly faster than repeatedly retrieving the data from S3, even if it's not as quick as having the whole dataframe in RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage of expensive objects or handles\n",
    "\n",
    "Sometimes we have an object, like a connection to a remote system, which cannot be serialized. In a pure, stateless task pattern, we'd recreate it as needed. But sometimes bending the rules is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import get_worker\n",
    "\n",
    "def worker_local_demo():\n",
    "    try:\n",
    "        local_val = get_worker().data['my_key']\n",
    "    except KeyError:    \n",
    "        get_worker().data['my_key'] = 'my_val'\n",
    "        return 'stored'\n",
    "    return local_val\n",
    "\n",
    "client.submit(worker_local_demo).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on where this task gets dispatched, you might have to run it twice to see `my_val` returned. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(worker_local_demo).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stored value is local to the worker process, but access is not necessarily thread safe. Thread-specific storage could be used via a `threading.local` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling output (result) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your output is small -- a report that can be viewed locally, or a set of model parameters -- then you may want to collect it to your local process via `.compute` or `.result`. \n",
    "\n",
    "But often we have a large result\n",
    "* we've filtered or joined large datasets and the result is large\n",
    "* we've performed an expensive transformation on array data but the result is still a very large array\n",
    "etc.\n",
    "\n",
    "In these cases, we do *not* want to retrieve the full result locally, but instead __write from our tasks in parallel to a destination__ like a shared filesystem, database, Kafka topic(s), etc.\n",
    "\n",
    "Here we'll find some NYC cab rides over $300. For this demo, there are only a few hundred such rides, but we might imagine that for many (lower) ride thresholds or larger fare datasets, the results could be too large to collect locally.\n",
    "\n",
    "For built-in collections, we'll use APIs like `.to_parquet`, `.to_zarr`, `.to_textfiles` and the like, which will write partition-wise results to separate output in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.environ['WRITE_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['total_amount']>300].to_csv('s3://' + bucket + '/expensive-rides')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes disconcerting to users that they end up with a bunch of `.part` files ... but this is usually what we want. This allows parallel writing and subsequent efficient parallel reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.read_csv('s3://' + bucket + '/expensive-rides/*').describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really need to combine the output .part files into a single file -- perhaps for consumption by another tool -- you can do this with \n",
    "* OS-level operations (`cat`)\n",
    "* filesystem-level helpers (HDFS `-getmerge`)\n",
    "* or a separate step in your workflow altogether (S3 doesn't support in-place merge) so that it doesn't bottleneck your work\n",
    "\n",
    "(Dask can write to a single CSV file with the `single_file` flag, but make sure you want that behavior and plan for the cost.)\n",
    "\n",
    "__Custom Code__\n",
    "\n",
    "For your custom code, emulating the patterns used in the Dask internal collections -- writing to a shared location with a separate file for each partition's output -- is a good default design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilience and Debugging\n",
    "\n",
    "### User code failure\n",
    "\n",
    "In the case of user code raising an exception, that exception is either re-raised on the local process, or is available for inspection (or re-raising) but will not cause the worker, independent tasks, or the Dask cluster to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "@delayed\n",
    "def calculate(n):\n",
    "    return n/0\n",
    "\n",
    "try:\n",
    "    calculate(10).compute()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker (process) failure\n",
    "\n",
    "Loss of a worker -- but not the nanny -- will result in the nanny restarting the worker, as we've seen.\n",
    "\n",
    "### Worker node/container/nanny/network failure\n",
    "\n",
    "In these situations, the scheduler will try to reschedule work on other workers, which will often allow a computation to succeed.\n",
    "\n",
    "> To get the best throughput, however, we may want to replace lost workers with new ones. The best way to ensure that new workers are created is to use the cluster's `.adapt` method. Even if we are not looking for a fully dynamic, scaling cluster, `adapt` can be used with `min` and `max` values to ensure that our worker pool remains within desired bounds.\n",
    "\n",
    "Even if workers are restored (or not needed), some there are some failure cases to watch out for.\n",
    "* If results depend on impure functions (e.g., a random value) then you may get a different result\n",
    "* If functions rely on side effects (e.g., looking at some OS/FS/container state value), then results are unpredictable\n",
    "* If the worker failed due to a bad function, for example a function that causes a segmentation fault, then that bad function will repeatedly be called on other workers.\n",
    "    * This function will be marked as “bad” after it kills a fixed number of workers (defaults to three).\n",
    "* Data sent out by user code to the workers via a call to `scatter()` (instead of being created from a Dask task graph via other Dask functions) may be irrecoverable by Dask (although user code may be able to supply it again).\n",
    "    * One way to \"harden\" the data availability in that scenario is via `Client.replicate` or to manually put it in resilient storage\n",
    "\n",
    "#### Avoiding \"worker cruft\" in long running clusters\n",
    "\n",
    "In a very long running cluster, we might occasionally build up state that endangers a worker. For example, the local scratch disk may fill up.\n",
    "\n",
    "Although it's an uncommon scenario, there are a couple of tools we can use in this situation:\n",
    "\n",
    "`client.restart()` will trigger a restart of all workers in a \"clean\" state -- this is also handy for troubleshooting or ensuring the cluster is in a known state before benchmarking new code.\n",
    "\n",
    "We might also programmatically retire workers in a long running cluster. The worker parameters enabling this are\n",
    "* `lifetime`\n",
    "* `lifetime_stagger`\n",
    "* `lifetime_restart`\n",
    "documented at https://distributed.dask.org/en/latest/worker.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler failure\n",
    "\n",
    "In a standard configuration, a scheduler failure is not recoverable. A new scheduler can be created, and existing workers may be able to re-connect to it, but in-progress computations (dependency graphs) are lost.\n",
    "\n",
    "> There is some experimental work on a multi-scheduler configuration which could potentially allow cluster users to continue to submit work. *But dependency graphs and other state in the failed scheduler are not replicated to other schedulers.* In other words, this is a load-balancing pattern that can minimize service interruptions, but it's not currently intended as a high-availability solution.\n",
    "> \n",
    "> More details are in this post: https://coiled.io/blog/dask-in-production-multi-scheduler-architectures/\n",
    "\n",
    "In a similar way, a \"higher-order\" cluster manager like a Kubernetes supervisor might also be able to create a new Dask scheduler on failure, but will not be able to restore lost scheduler state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Stealing\n",
    "\n",
    "Work stealing is the process whereby the scheduler removes work from one worker and assigns it to another. Work stealing is really about performance rather than resilience. But inasmuch as it can improve the end-to-end effectiveness of Dask applied to your problems over time, and aims to minimize overloading individual workers, it helps in many areas.\n",
    "\n",
    "### Why would things become imbalanced in the first place?\n",
    "\n",
    "Task placement is done largely to support data locality, or minimizing the transport of data dependencies. This is generally helpful, but can mean assigning a lot of tasks, or long-running tasks, to a few workers that have critical data.\n",
    "\n",
    "When the imbalance is big enough, it makes sense to re-assign tasks to an idle worker, rather than wait for the \"star workers\" to crank through all of their assigned work.\n",
    "\n",
    "Work stealing is largely automatic (detailed at https://distributed.dask.org/en/latest/work-stealing.html) but it's another situation where we can help be ensuring our data is available as widely as possible -- e.g., by storing large intermediate results in fast shared storage. Whether that (which will also incur costs) is worth it depends a lot on the details of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "Debugging at-scale compute jobs is challenging, for a variety of reasons.\n",
    "\n",
    "* Analyzing parallel computations is hard in general\n",
    "* Scale adds additional sources of unpredictability\n",
    "    * On a single machine, we worry about state and timing\n",
    "    * With multiple machines we add network issues, storage issues, and additional hardware variables\n",
    "* Problematic data records (the ones that cause a failure) may be a tiny fraction of a huge dataset, making them expensive to find\n",
    "* System-level failures (e.g., memory issues) may be dependent on\n",
    "    * prior jobs or other simultaneous jobs\n",
    "    * how those other jobs are/were distributed\n",
    "    * the physical location of replicas of data used in a job\n",
    "    \n",
    "For repeated, batch type jobs, a common source of unpredictable bugs in production is the change in data selected (and its distribution).\n",
    "* Tuning for Monday's job may not hold up for Friday's data\n",
    "* If your storage and Dask workers are colocated, locations of needed replicas in local distributed storage (e.g., HDFS) may be different relative to placement of Dask workers across runs\n",
    "\n",
    "## \"Easy Level\": application logic errors\n",
    "\n",
    "The easiest errors to debug are errors in specific application logic. These could be errors in our custom functions or even bugs in Dask code.\n",
    "\n",
    "Because these are deterministic and less dependent on the data and environment, we can try to reproduce these errors and then fix them.\n",
    "\n",
    "We've seen that a failed Future makes its exception available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.submit(lambda x:x/0, 5).result()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception and traceback are also available directly (i.e., without raising the error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = client.submit(lambda x:x/0, 5)\n",
    "f.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f.traceback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed future errors can be programmatically raised for local debugging. Notice that the traceback here is slightly different, which may be helpful for some errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.recreate_error_locally(f)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For application-code errors that don't require full-scale operations to reproduce, we can also try to re-create them in smaller, easier to debug settings.\n",
    "\n",
    "* `.compute(scheduler='single-threaded')`\n",
    "* `LocalCluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed troubleshooting\n",
    "\n",
    "For \"big-data, big-cluster\" error cases -- where you cannot simply reproduce the error in a limited environment -- we will need to narrow the problem down by a combination of \n",
    "* filtering data (to locate problematic records or dataset size thresholds)\n",
    "* inspecting worker reports and dashboards\n",
    "* reading worker logs\n",
    "\n",
    "From the main Dask dashboard, the __Info__ tab provides a list of workers along with links to each worker's\n",
    "* memory and call stack data\n",
    "* logs\n",
    "* realtime (animated) dashboard\n",
    "\n",
    "These tools can give further insight into what is going wrong, particularly when we're seeing a resource exhaustion scenario where our job fails or takes indefinitely long to make progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for running arbitrary code on workers -- outside of the scheduler/task framework, typically for inspecting or mutating the worker environment, we have `client.run`\n",
    "\n",
    "`client.run` allows us to run a function on all workers, or on a specific list of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "client.run(os.cpu_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a shortcut for accessing the worker itself. Instead of using `get_worker()`, you can create a dummy argument called `dask_worker` which will be populated by the worker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "delayed([1,2,3]).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "\n",
    "def get_data(dask_worker):\n",
    "    return [k for k in dask_worker.data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(get_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and don't forget, before assuming things about the current config, to take a look via the `dask.config` APIs!\n",
    "\n",
    "# Wrap-up\n",
    "\n",
    "It would be disingenuous to pretend that any tool will make distributed, big-data debugging simple... and even more disingenuous to pretend we won't encounter any problems at all.\n",
    "\n",
    "But, relative to the power and breadth of functionality, Dask offers an extremely strong suite of tools for seeing inside your living, breathing cluster while it's running, and performing diagnostic inspections when things don't go right.\n",
    "\n",
    "More importantly, the best tool for avoiding distributed data headaches is attention to best practices in design.\n",
    "\n",
    "> As we've highlighted earlier, there are a variety of patterns for distributing work and data which may not be obvious or necessary in local or small-scale computation, but become critical defining factors for successful large-scale operations.\n",
    "\n",
    "Dask helps us by doing a solid job of \"making it easier to do the right thing and harder to do the wrong thing\" and if we supplement that with a thoughtful design, code as simple as possible, and sensible patterns, we can avoid, rather than confront, most hassles... and that will let us make more progress on the end goal for which we are processing data in the first place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
